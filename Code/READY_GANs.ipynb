{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "READY - GANs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ult0blXNbHtv",
        "colab_type": "text"
      },
      "source": [
        "** MODEL SIZE **\n",
        "\n",
        "<hr>\n",
        "\n",
        "*Needs to set*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70kWNWiMbHdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generator hidden size\n",
        "model_sizes = [128, 256, 512]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIyt-AKk1biq",
        "colab_type": "text"
      },
      "source": [
        "# **GANs for Abstractive Text Summarization**\n",
        "## **NLP Group Project**\n",
        "## **Statistical Natural Language Processing (COMP0087), University College London**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Project description**\n",
        "\n",
        "A lot of endeavours have already been devoted to NLP text summarization techniques, and abstractive methods have proved to be more proficient in generating human-like sentences. At the same time, GANs has been enjoying considerable success in the area of real-valued data such as an image generation. Recently, researchers have begun to come up with ideas on how to overcome various obstacles during training GAN models for discrete data, though not a lot of work seemed to be directly dedicated to the text summarization itself. We, therefore, would like to pursue to tackle the issue of text summarization using the GAN techniques inspired by sources enlisted below.\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Collaborators**\n",
        "\n",
        "- Daniel Stancl (daniel.stancl.19@ucl.ac.uk)\n",
        "- Dorota Jagnesakova (dorota.jagnesakova.19@ucl.ac.uk)\n",
        "- Guolinag HE (guoliang.he.19@ucl.ac.uk)\n",
        "- Zakhar Borok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzNR-0NJoCOV",
        "colab_type": "text"
      },
      "source": [
        "# **1 Setup**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- install and import libraries\n",
        "- download stopwords\n",
        "- remove and clone the most recent version of git repository\n",
        "- run a script with a CONTRACTION_MAP\n",
        "- run a script with a function for text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRRMxZxggWVl",
        "colab_type": "text"
      },
      "source": [
        "### **GitHub stuff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkaEqWkNphsX",
        "colab_type": "text"
      },
      "source": [
        "**Set GitHub credentials and username of repo owner**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0r58gXnphct",
        "colab_type": "code",
        "outputId": "267de2de-750d-4724-9876-7e7043d7faa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# credentials\n",
        "user_email = 'dannyi@seznam.cz'\n",
        "user = \"gansforlife\"\n",
        "user_password = \"dankodorkamichaelzak\"\n",
        "\n",
        "# username of repo owner\n",
        "owner_username = 'stancld'\n",
        "# reponame\n",
        "reponame = 'GeneratingHeadline_GANs'\n",
        "\n",
        "# generate \n",
        "add_origin_link = (\n",
        "    'https://{}:{}github@github.com/{}/{}.git'.format(\n",
        "    user, user_password, owner_username, reponame)\n",
        ")\n",
        "\n",
        "print(\"Link used for git cooperation:\\n{}\".format(add_origin_link))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Link used for git cooperation:\n",
            "https://gansforlife:dankodorkamichaelzakgithub@github.com/stancld/GeneratingHeadline_GANs.git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izq91t63kr2l",
        "colab_type": "text"
      },
      "source": [
        "**Clone GitHub repo on the personal drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va3lSdh1ycMI",
        "colab_type": "code",
        "outputId": "16fc64bc-c9d6-480b-ff24-e13550ef0567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "## Clone GitHub repo to the desired folder\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "%cd \"drive/My Drive/projects\"\n",
        "\n",
        "# Remove NLP_Project if presented and clone up-to-date repo\n",
        "!rm -r GeneratingHeadline_GANs\n",
        "!git clone https://github.com/stancld/GeneratingHeadline_GANs.git\n",
        "\n",
        "# Go to the NLP_Project folder\n",
        "%cd GeneratingHeadline_GANs\n",
        "\n",
        "# Config global user and add origin enabling us to execute push commands\n",
        "!git config --global user.email user_email\n",
        "!git remote rm origin\n",
        "!git remote add origin https://gansforlife:dankodorkamichaelzakgithub@github.com/stancld/GeneratingHeadline_GANs.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/projects\n",
            "Cloning into 'GeneratingHeadline_GANs'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 1293 (delta 7), reused 18 (delta 5), pack-reused 1273\u001b[K\n",
            "Receiving objects: 100% (1293/1293), 17.57 MiB | 16.49 MiB/s, done.\n",
            "Resolving deltas: 100% (722/722), done.\n",
            "Checking out files: 100% (104/104), done.\n",
            "/content/drive/My Drive/projects/GeneratingHeadline_GANs\n",
            "CPU times: user 236 ms, sys: 90.5 ms, total: 327 ms\n",
            "Wall time: 10.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtd82jejgjsJ",
        "colab_type": "text"
      },
      "source": [
        "**Function push_to_repo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQKbDCp2gZs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def push_to_repo():\n",
        "  \"\"\"\n",
        "  models_branch\n",
        "  \"\"\"\n",
        "  !git remote rm origin\n",
        "  !git remote add origin https://gansforlife:dankodorkamichaelzak@github.com/stancld/GeneratingHeadline_GANs.git\n",
        "  !git checkout master\n",
        "  !git pull origin master\n",
        "  !git checkout models_branch\n",
        "  !git add .\n",
        "  !git commit -m \"model state update\"\n",
        "  !git checkout master\n",
        "  !git merge models_branch\n",
        "  !git push -u origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSITDO2N88Mu",
        "colab_type": "text"
      },
      "source": [
        "### **General stuff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQViomvJmmxy",
        "colab_type": "text"
      },
      "source": [
        "**Import essential libraries and load necessary conditionalities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLGRKAbp3qo1",
        "colab_type": "code",
        "outputId": "65805751-9d2a-4e55-fc00-a165a716b4b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pip install rouge"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfSL4JZGoXH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import copy\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from rouge import Rouge\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9EI-8lbFAfk",
        "colab_type": "code",
        "outputId": "114acc89-1c4e-4c4c-886d-7c950ae593d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9csn_FoXmqMR",
        "colab_type": "text"
      },
      "source": [
        "**Set essential parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpfj2zejmsaV",
        "colab_type": "code",
        "outputId": "632dbbe4-51a4-4347-9d47-ca92b675c782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Set torch.device to use GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.get_device_name())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJyjpXDMo-HN",
        "colab_type": "text"
      },
      "source": [
        "**Run python files from with classes used throughtout the document**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhIgA7osp7Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run Code/contractions.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb3DjRcT1gm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for text_preprocessing()\n",
        "run Code/text_preprocessing.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ0x45Yzqggm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for transforming data to padded array\n",
        "run Code/data2PaddedArray.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBm-0p9IS9yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the generator\n",
        "run Code/Models/Attention_seq2seq.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrL0zprqccwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the training class (generator)\n",
        "run Code/Models/generator_training_class.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xJ0OlkZqw57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the discriminator\n",
        "run Code/Models/CNN_text_clf.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uduYkgESqwsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the training class (generator)\n",
        "run Code/Models/discriminator_training_class.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZMHJLms80hC",
        "colab_type": "text"
      },
      "source": [
        "### **Pretrained embeddings**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**TODO:** *Put a comment which kind of embeddings we used. Add some references and so on*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfPzKeHh80DB",
        "colab_type": "code",
        "outputId": "46c18b73-1e1c-4609-ba01-659ba90d810a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "embed_dim = 200\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Download and unzip GloVe embedding\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip glove.6B.zip\n",
        "\n",
        "\n",
        "# input your pre-train txt path and parse the data\n",
        "path = '../data/glove.6B.{:.0f}d.txt'.format(embed_dim)\n",
        "\n",
        "# for michael\n",
        "# path = r'/content/drive/My Drive/glove.6B.{:.0f}d.txt'.format(embed_dim)\n",
        "\n",
        "embed_dict = {}\n",
        "with open(path,'r') as f:\n",
        "  lines = f.readlines()\n",
        "  for l in lines:\n",
        "    w = l.split()[0]\n",
        "    v = np.array(l.split()[1:]).astype('float')\n",
        "    embed_dict[w] = v\n",
        "\n",
        "embed_dict['@@_unknown_@@'] = np.random.random(embed_dim)\n",
        "\n",
        "# remove all the unnecesary files\n",
        "#!rm -rf glove.6B.zip\n",
        "#!rm -rf glove.6B.50d.txt\n",
        "#!rm -rf glove.6B.100d.txt\n",
        "#!rm -rf glove.6B.200d.txt\n",
        "#!rm -rf glove.6B.300d.txt\n",
        "\n",
        "# check the length of the dictionary\n",
        "len(embed_dict.keys())\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n# Download and unzip GloVe embedding\\n#!wget http://nlp.stanford.edu/data/glove.6B.zip\\n#!unzip glove.6B.zip\\n\\n\\n# input your pre-train txt path and parse the data\\npath = '../data/glove.6B.{:.0f}d.txt'.format(embed_dim)\\n\\n# for michael\\n# path = r'/content/drive/My Drive/glove.6B.{:.0f}d.txt'.format(embed_dim)\\n\\nembed_dict = {}\\nwith open(path,'r') as f:\\n  lines = f.readlines()\\n  for l in lines:\\n    w = l.split()[0]\\n    v = np.array(l.split()[1:]).astype('float')\\n    embed_dict[w] = v\\n\\nembed_dict['@@_unknown_@@'] = np.random.random(embed_dim)\\n\\n# remove all the unnecesary files\\n#!rm -rf glove.6B.zip\\n#!rm -rf glove.6B.50d.txt\\n#!rm -rf glove.6B.100d.txt\\n#!rm -rf glove.6B.200d.txt\\n#!rm -rf glove.6B.300d.txt\\n\\n# check the length of the dictionary\\nlen(embed_dict.keys())\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_doqN6K-L_f",
        "colab_type": "text"
      },
      "source": [
        "**Function for extracting relevant matrix of pretrained weights** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwSHzoth-LmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_weight(text_dictionary):\n",
        "  \"\"\"\n",
        "  :param text_dictionary:\n",
        "  \"\"\"\n",
        "  pre_train_weight = []\n",
        "  for word_index in text_dictionary.index2word.keys():\n",
        "    if word_index != 0:\n",
        "      word = text_dictionary.index2word[word_index]\n",
        "      try:\n",
        "        word_vector = embed_dict[word].reshape(1,-1)\n",
        "      except:\n",
        "        word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word\n",
        "      pre_train_weight = np.vstack([pre_train_weight,word_vector])\n",
        "    \n",
        "    # add for padding\n",
        "    elif word_index == len(text_dictionary.index2word.keys()):  \n",
        "      pre_train_weight = np.r_[pre_train_weight, np.zeros((1, embed_dim))]\n",
        "    \n",
        "    else:\n",
        "      word = text_dictionary.index2word[word_index]\n",
        "      try:\n",
        "        word_vector = embed_dict[word].reshape(1,-1)\n",
        "      except:\n",
        "        word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word\n",
        "      pre_train_weight = word_vector\n",
        "  return pre_train_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25PaDFvYp8VY",
        "colab_type": "text"
      },
      "source": [
        "# **2 Load and process the data**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Source of the data:** https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1AQ45P8lRzX",
        "colab_type": "text"
      },
      "source": [
        "##### *Open and preprocess the data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRY-oCuJLC-u",
        "colab_type": "code",
        "outputId": "1848aeb9-724f-4947-d0a3-2275a0d8d0d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "%%time\n",
        "# Open\n",
        "data = pd.read_csv('../data/wikihowSep.csv',\n",
        "                    error_bad_lines = False).astype(str)\n",
        "print(data.shape)\n",
        "\n",
        "# Preprocess\n",
        "for item in ['text', 'headline']:\n",
        "  exec(\"{}_data = text_preprocessing(data=data, item = '{}', contraction_map=CONTRACTION_MAP, drop_digits=False, remove_stopwords=False, stemming=False).format(item, item), locals(), globals()\" )\n",
        "\n",
        "# Cleaning\n",
        "del data\n",
        "gc.collect()\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n%%time\\n# Open\\ndata = pd.read_csv(\\'../data/wikihowSep.csv\\',\\n                    error_bad_lines = False).astype(str)\\nprint(data.shape)\\n\\n# Preprocess\\nfor item in [\\'text\\', \\'headline\\']:\\n  exec(\"{}_data = text_preprocessing(data=data, item = \\'{}\\', contraction_map=CONTRACTION_MAP, drop_digits=False, remove_stopwords=False, stemming=False).format(item, item), locals(), globals()\" )\\n\\n# Cleaning\\ndel data\\ngc.collect()\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn-AsxLZVvOD",
        "colab_type": "text"
      },
      "source": [
        "##### *Clean flawed examples*\n",
        "\n",
        "<hr>\n",
        "\n",
        "- drop examples based on the threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw5hZUiJKIW4",
        "colab_type": "code",
        "outputId": "cfa64045-f17b-4ce7-d110-f85609ff8b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# drop examples with an invalid ratio of length of text and headline\n",
        "text_len = [len(t) for t in text_data]\n",
        "head_len = [len(h) for h in headline_data]\n",
        "\n",
        "print('Some statistics')\n",
        "\n",
        "print('Average length of articles is {:.2f}.'.format(np.array(text_len).mean()))\n",
        "print('Min = {:.0f}, Max = {:.0f}, Std = {:.2f}'.format(min(text_len), max(text_len), np.array(text_len).std()))\n",
        "\n",
        "print('-----')\n",
        "\n",
        "print('Average length of summaries is {:.2f}.'.format(np.array(head_len).mean()))\n",
        "print('Min = {:.0f}, Max = {:.0f}, Std = {:.2f}'.format(min(head_len), max(head_len), np.array(head_len).std()))\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n# drop examples with an invalid ratio of length of text and headline\\ntext_len = [len(t) for t in text_data]\\nhead_len = [len(h) for h in headline_data]\\n\\nprint('Some statistics')\\n\\nprint('Average length of articles is {:.2f}.'.format(np.array(text_len).mean()))\\nprint('Min = {:.0f}, Max = {:.0f}, Std = {:.2f}'.format(min(text_len), max(text_len), np.array(text_len).std()))\\n\\nprint('-----')\\n\\nprint('Average length of summaries is {:.2f}.'.format(np.array(head_len).mean()))\\nprint('Min = {:.0f}, Max = {:.0f}, Std = {:.2f}'.format(min(head_len), max(head_len), np.array(head_len).std()))\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKyHNUTrWNvL",
        "colab_type": "code",
        "outputId": "1a83070a-b1f1-4f1b-9a20-45bdde3e552d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "max_examples = 150000\n",
        "max_threshold = 0.75\n",
        "\n",
        "# drop examples with an invalid ratio of length of text and headline\n",
        "text_len = [len(t) for t in text_data]\n",
        "head_len = [len(h) for h in headline_data]\n",
        "\n",
        "ratio = [h/t for t, h in zip(text_len, head_len)]\n",
        "\n",
        "problems1 = [problem for problem, r in enumerate(ratio) if (r > max_threshold)]\n",
        "print(len(problems1))\n",
        "text_data, headline_data = np.delete(text_data, problems1), np.delete(headline_data, problems1)\n",
        "print(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\n",
        "\n",
        "# drop too long articles (to avoid struggles with CUDA memory) and too short\n",
        "text_len = [len(t) for t in text_data]\n",
        "\n",
        "problems2 = [problem for problem, text_length in enumerate(text_len) if ((text_length > 200) | (text_length < 10) )]\n",
        "print(len(problems2))\n",
        "text_data, headline_data = np.delete(text_data, problems2), np.delete(headline_data, problems2)\n",
        "print(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\n",
        "\n",
        "# drop too pairs with too short/long summaries\n",
        "head_len = [len(h) for h in headline_data]\n",
        "\n",
        "problems3 = [problem for problem, headline_len in enumerate(head_len) if ( (headline_len > 75) | (headline_len < 2) )]\n",
        "print(len(problems3))\n",
        "text_data, headline_data = np.delete(text_data, problems3), np.delete(headline_data, problems3)\n",
        "print(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\n",
        "\n",
        "# some cleaning\n",
        "del text_len, head_len, ratio, problems1, problems2, problems3\n",
        "gc.collect()\n",
        "\n",
        "# trim the data to have only a subset of the data for our project\n",
        "try:\n",
        "  text_data, headline_data = text_data[:max_examples], headline_data[:max_examples]\n",
        "except:\n",
        "  pass\n",
        "\n",
        "print(text_data.shape, headline_data.shape)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nmax_examples = 150000\\nmax_threshold = 0.75\\n\\n# drop examples with an invalid ratio of length of text and headline\\ntext_len = [len(t) for t in text_data]\\nhead_len = [len(h) for h in headline_data]\\n\\nratio = [h/t for t, h in zip(text_len, head_len)]\\n\\nproblems1 = [problem for problem, r in enumerate(ratio) if (r > max_threshold)]\\nprint(len(problems1))\\ntext_data, headline_data = np.delete(text_data, problems1), np.delete(headline_data, problems1)\\nprint(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\\n\\n# drop too long articles (to avoid struggles with CUDA memory) and too short\\ntext_len = [len(t) for t in text_data]\\n\\nproblems2 = [problem for problem, text_length in enumerate(text_len) if ((text_length > 200) | (text_length < 10) )]\\nprint(len(problems2))\\ntext_data, headline_data = np.delete(text_data, problems2), np.delete(headline_data, problems2)\\nprint(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\\n\\n# drop too pairs with too short/long summaries\\nhead_len = [len(h) for h in headline_data]\\n\\nproblems3 = [problem for problem, headline_len in enumerate(head_len) if ( (headline_len > 75) | (headline_len < 2) )]\\nprint(len(problems3))\\ntext_data, headline_data = np.delete(text_data, problems3), np.delete(headline_data, problems3)\\nprint(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\\n\\n# some cleaning\\ndel text_len, head_len, ratio, problems1, problems2, problems3\\ngc.collect()\\n\\n# trim the data to have only a subset of the data for our project\\ntry:\\n  text_data, headline_data = text_data[:max_examples], headline_data[:max_examples]\\nexcept:\\n  pass\\n\\nprint(text_data.shape, headline_data.shape)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FooWzaiyVnum",
        "colab_type": "text"
      },
      "source": [
        "**LOAD ALREADY PREPROCESSED DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlDEwTJJUKtA",
        "colab_type": "code",
        "outputId": "63981c00-c430-435b-e1e9-97b0fb8d0411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "text_data = np.load('../data/text_data.npy', allow_pickle = True)\n",
        "headline_data = np.load('../data/headline_data.npy', allow_pickle = True)\n",
        "\n",
        "print(text_data.shape, headline_data.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150000,) (150000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KKBqq0oIgey",
        "colab_type": "text"
      },
      "source": [
        "*Print some statistics*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKHJ_IiUIf8I",
        "colab_type": "code",
        "outputId": "79ed7eab-38d1-4c6d-a716-22955c717f94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "# drop examples with an invalid ratio of length of text and headline\n",
        "text_len = [len(t) for t in text_data]\n",
        "head_len = [len(h) for h in headline_data]\n",
        "\n",
        "print('Some statistics')\n",
        "\n",
        "print('Average length of articles is {:.2f}.'.format(np.array(text_len).mean()))\n",
        "print('Min = {:.0f}, Max = {:.0f}, Std = {:.2f}'.format(min(text_len), max(text_len), np.array(text_len).std()))\n",
        "\n",
        "print('-----')\n",
        "\n",
        "print('Average length of summaries is {:.2f}.'.format(np.array(head_len).mean()))\n",
        "print('Min = {:.0f}, Max = {:.0f}, Std = {:.2f}'.format(min(head_len), max(head_len), np.array(head_len).std()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some statistics\n",
            "Average length of articles is 87.47.\n",
            "Min = 10, Max = 200, Std = 42.66\n",
            "-----\n",
            "Average length of summaries is 9.45.\n",
            "Min = 3, Max = 68, Std = 4.49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBkI74Qa9Y9v",
        "colab_type": "text"
      },
      "source": [
        "##### *Split data into train/val/test set*\n",
        "\n",
        "<hr>\n",
        "\n",
        "It's crucial to do this split in this step so that a dictionary that will be created for our model won't contain any words from validation/test set which are not presented in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsXTsEEV8851",
        "colab_type": "code",
        "outputId": "15cdcec5-6b98-422f-a415-d55e405f8662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "np.random.seed(222)\n",
        "\n",
        "split = np.random.uniform(0, 1, size = text_data.shape[0])\n",
        "\n",
        "# Train set\n",
        "text_train, headline_train = text_data[split <= 0.9], headline_data[split <= 0.9]\n",
        "# Validation set\n",
        "text_val, headline_val = text_data[(split > 0.9) & (split <= 0.95)], headline_data[(split > 0.9) & (split <= 0.95)]\n",
        "# Test set\n",
        "text_test, headline_test = text_data[split > 0.95], headline_data[split > 0.95]\n",
        "\n",
        "del text_data, headline_data\n",
        "gc.collect()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a14LfijYE6bP",
        "colab_type": "text"
      },
      "source": [
        "*Print some statistics*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HGCRWtBE5r_",
        "colab_type": "code",
        "outputId": "a8619f4d-4735-4395-f8f4-8d30d57e17c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Average lengths of articles is {:.2f}'.format(np.array([len(text) for text in text_train]).mean()))\n",
        "\n",
        "print('Average lengths of sumaries is {:.2f}'.format(np.array([len(text) for text in headline_train]).mean()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average lengths of articles is 87.39\n",
            "Average lengths of sumaries is 9.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0nqBWooWsRv",
        "colab_type": "text"
      },
      "source": [
        "##### *Sort dataset from the longest sequence to the shortest one*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4tHv5huWss9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_data(text, headline):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  headline = np.array(\n",
        "      [y for x,y in sorted(zip(text, headline), key = lambda pair: len(pair[0]), reverse = True)]\n",
        "  )\n",
        "  text = list(text)\n",
        "  text.sort(key = lambda x: len(x), reverse = True)\n",
        "  text = np.array(text)\n",
        "\n",
        "  return text, headline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olWbPN8eY8j9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train set\n",
        "text_train, headline_train = sort_data(text_train, headline_train)\n",
        "# Validation set\n",
        "text_val, headline_val = sort_data(text_val, headline_val)\n",
        "# Test set\n",
        "text_test, headline_test = sort_data(text_test, headline_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TLaKPwlFHdu",
        "colab_type": "text"
      },
      "source": [
        "### **Prepare dictionary and embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dfcRmfIN5Yc",
        "colab_type": "text"
      },
      "source": [
        "##### *Create a dictionary and prepare a digestible representation of the data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbH1F24FOOhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LangDict:\n",
        "  \"\"\"\n",
        "  Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "    self.n_words = 2\n",
        "\n",
        "  def add_article(self, article):\n",
        "    for word in article:\n",
        "      self.add_word(word)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p5hFIFLQz_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dictionary based on the training data\n",
        "text_dictionary = LangDict()\n",
        "headline_dictionary = LangDict()\n",
        "\n",
        "for article in text_train:\n",
        "  text_dictionary.add_article(article)\n",
        "\n",
        "for article in headline_train:\n",
        "  headline_dictionary.add_article(article)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IULBuowRDHM",
        "colab_type": "code",
        "outputId": "aefd1713-6d35-4925-9554-bc88d36ef6ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "print(\"There are {:.0f} distinct words in the untrimmed text dictionary\".format(len(text_dictionary.word2index.keys())))\n",
        "print(\"There are {:.0f} distinct words in the untrimmed headline dictionary\".format(len(headline_dictionary.word2index.keys())))\n",
        "\n",
        "# Trim a dictionary to the words with at least 10 occurences within the text\n",
        "text_min_count = 1\n",
        "head_min_count = 2\n",
        "\n",
        "## TEXT DICTIONARY\n",
        "subset_words = [word for (word, count) in text_dictionary.word2count.items() if count >= text_min_count]\n",
        "text_dictionary.word2index = {word: i for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "text_dictionary.index2word = {i: word for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "text_dictionary.word2count = {word: count for (word, count) in text_dictionary.word2count.items() if count >= text_min_count}\n",
        "\n",
        "## HEADLINE DICTIONARY\n",
        "subset_words = [word for (word, count) in headline_dictionary.word2count.items() if count >= head_min_count]\n",
        "headline_dictionary.word2index = {word: i for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "headline_dictionary.index2word = {i: word for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "headline_dictionary.word2count = {word: count for (word, count) in headline_dictionary.word2count.items() if count >= head_min_count}\n",
        "\n",
        "print(\"There are {:.0f} distinct words in the trimmed text dictionary, where only word with at least {:.0f} occurences are retained\".format(len(text_dictionary.word2index.keys()), text_min_count))\n",
        "print(\"There are {:.0f} distinct words in the trimmed headline dictionary, where only word with at least {:.0f} occurences are retained\".format(len(headline_dictionary.word2index.keys()), head_min_count))\n",
        "del text_min_count, head_min_count, subset_words"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 67860 distinct words in the untrimmed text dictionary\n",
            "There are 23368 distinct words in the untrimmed headline dictionary\n",
            "There are 67860 distinct words in the trimmed text dictionary, where only word with at least 1 occurences are retained\n",
            "There are 15049 distinct words in the trimmed headline dictionary, where only word with at least 2 occurences are retained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "992ZzrRcoK7d",
        "colab_type": "text"
      },
      "source": [
        "*Add pad token*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_TTnlQ5oKl2",
        "colab_type": "code",
        "outputId": "0a1ed955-84ab-489d-ffcf-38989355d611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "## TEXT DICTIONARY\n",
        "pad_idx = max(list(text_dictionary.index2word.keys())) + 1\n",
        "\n",
        "text_dictionary.word2index['<pad>'] = pad_idx\n",
        "text_dictionary.index2word[pad_idx] = '<pad>'\n",
        "\n",
        "print(len(text_dictionary.index2word.keys()))\n",
        "\n",
        "## HEADLINE DICTIONARY\n",
        "pad_idx = max(list(headline_dictionary.index2word.keys())) + 1\n",
        "\n",
        "headline_dictionary.word2index['<pad>'] = pad_idx\n",
        "headline_dictionary.index2word[pad_idx] = '<pad>'\n",
        "\n",
        "print(len(headline_dictionary.index2word.keys()))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "67861\n",
            "15050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGQcSyqKHF5l",
        "colab_type": "text"
      },
      "source": [
        "##### *Extract embedding vectors for words we need*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iTKyHy0lQvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "%%time\n",
        "pre_train_weight = extract_weight(text_dictionary)\n",
        "pre_train_weight = np.array(pre_train_weight, dtype = np.float32)\n",
        "np.save('../data/embedding.npy', pre_train_weight)\n",
        "\n",
        "pre_train_weight_head = extract_weight(headline_dictionary)\n",
        "pre_train_weight_head = np.array(pre_train_weight_head, dtype = np.float32)\n",
        "np.save('../data/embedding_headline.npy', pre_train_weight_head)\n",
        "\n",
        "del embed_dict\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "pre_train_weight = np.load('../data/embedding.npy')\n",
        "pre_train_weight_head = np.load('../data/embedding_headline.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHL6SrbtjSZX",
        "colab_type": "text"
      },
      "source": [
        "### **Transform the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONMMcJytjS47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train set\n",
        "text_train, text_lengths_train, headline_train, headline_lengths_train = data2PaddedArray(text_train, headline_train, {'text_dictionary': text_dictionary,\n",
        "                                                                                                                       'headline_dictionary': headline_dictionary},\n",
        "                                                                                          pre_train_weight)\n",
        "# Validation set\n",
        "text_val, text_lengths_val, headline_val, headline_lengths_val = data2PaddedArray(text_val, headline_val, {'text_dictionary': text_dictionary,\n",
        "                                                                                                           'headline_dictionary': headline_dictionary},\n",
        "                                                                                  pre_train_weight)\n",
        "# Test set\n",
        "text_test, text_lengths_test, headline_test, headline_lengths_test = data2PaddedArray(text_test, headline_test, {'text_dictionary': text_dictionary,\n",
        "                                                                                                                 'headline_dictionary': headline_dictionary},\n",
        "                                                                                       pre_train_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N4c4riOR_Ck",
        "colab_type": "text"
      },
      "source": [
        "# **3 Training**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF6q63I5XpZN",
        "colab_type": "text"
      },
      "source": [
        "## **3.1 Generator - Pretraining**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Description**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUkoLDKlmDkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "62adae6a-6a80-4225-c06d-d4da96816746"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "for model_size in model_sizes:\n",
        "  # Model specification\n",
        "  grid = {'max_epochs': 25,\n",
        "          'batch_size': 32,\n",
        "          'learning_rate': 3e-4,\n",
        "          'clip': 10,\n",
        "          'l2_reg': 1e-4,\n",
        "          'model_name': \"generator{:.0f}\".format(model_size)\n",
        "        }\n",
        "\n",
        "  ##### model ######\n",
        "  OUTPUT_DIM = len(headline_dictionary.index2word.keys())\n",
        "  ENC_EMB_DIM = pre_train_weight.shape[1]\n",
        "  ENC_HID_DIM = model_size\n",
        "  DEC_HID_DIM = model_size\n",
        "\n",
        "  enc_num_layers = 1 # number of layers in RNN\n",
        "  dec_num_layers = 1 # number of layers in RNN\n",
        "\n",
        "  ENC_DROPOUT = 0.1\n",
        "  DEC_DROPOUT = 0.1\n",
        "\n",
        "  # Initialization\n",
        "  Generator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid['l2_reg'], batch_size = grid['batch_size'],\n",
        "                      text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid['max_epochs'], learning_rate = grid['learning_rate'],\n",
        "                      clip = grid['clip'], teacher_forcing_ratio = 1, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\n",
        "                      DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, enc_num_layers = enc_num_layers, dec_num_layers = dec_num_layers,\n",
        "                      device = device, model_name = grid['model_name'], push_to_repo = push_to_repo)\n",
        "  \n",
        "  # Load model if any\n",
        "  Generator.load()\n",
        "\n",
        "  # Prin model design and total number of parameters\n",
        "  print(Generator.model)\n",
        "  model_parameters = filter(lambda p: p.requires_grad, Generator.model.parameters())\n",
        "  params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "  print(\"Model with RNN hidden size of {:.0f} has {:.0f} parameters in total.\".format(model_size, params))\n",
        "\n",
        "  # Run training\n",
        "  Generator.train(X_train = text_train,\n",
        "                y_train = headline_train,\n",
        "                X_val = text_val,\n",
        "                y_val = headline_val,\n",
        "                X_train_lengths = text_lengths_train,\n",
        "                y_train_lengths = headline_lengths_train,\n",
        "                X_val_lengths = text_lengths_val,\n",
        "                y_val_lengths = headline_lengths_val)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nfor model_size in model_sizes:\\n  # Model specification\\n  grid = {\\'max_epochs\\': 25,\\n          \\'batch_size\\': 32,\\n          \\'learning_rate\\': 3e-4,\\n          \\'clip\\': 10,\\n          \\'l2_reg\\': 1e-4,\\n          \\'model_name\\': \"generator{:.0f}\".format(model_size)\\n        }\\n\\n  ##### model ######\\n  OUTPUT_DIM = len(headline_dictionary.index2word.keys())\\n  ENC_EMB_DIM = pre_train_weight.shape[1]\\n  ENC_HID_DIM = model_size\\n  DEC_HID_DIM = model_size\\n\\n  enc_num_layers = 1 # number of layers in RNN\\n  dec_num_layers = 1 # number of layers in RNN\\n\\n  ENC_DROPOUT = 0.1\\n  DEC_DROPOUT = 0.1\\n\\n  # Initialization\\n  Generator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid[\\'l2_reg\\'], batch_size = grid[\\'batch_size\\'],\\n                      text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid[\\'max_epochs\\'], learning_rate = grid[\\'learning_rate\\'],\\n                      clip = grid[\\'clip\\'], teacher_forcing_ratio = 1, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\\n                      DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, enc_num_layers = enc_num_layers, dec_num_layers = dec_num_layers,\\n                      device = device, model_name = grid[\\'model_name\\'], push_to_repo = push_to_repo)\\n  \\n  # Load model if any\\n  Generator.load()\\n\\n  # Prin model design and total number of parameters\\n  print(Generator.model)\\n  model_parameters = filter(lambda p: p.requires_grad, Generator.model.parameters())\\n  params = sum([np.prod(p.size()) for p in model_parameters])\\n  print(\"Model with RNN hidden size of {:.0f} has {:.0f} parameters in total.\".format(model_size, params))\\n\\n  # Run training\\n  Generator.train(X_train = text_train,\\n                y_train = headline_train,\\n                X_val = text_val,\\n                y_val = headline_val,\\n                X_train_lengths = text_lengths_train,\\n                y_train_lengths = headline_lengths_train,\\n                X_val_lengths = text_lengths_val,\\n                y_val_lengths = headline_lengths_val)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ArdeuG0IWsh",
        "colab_type": "text"
      },
      "source": [
        "*Plot losses*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWSzomoy3NcM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "4614ab70-716c-40af-83cc-132b7bdc0e09"
      },
      "source": [
        "fig, ax = plt.subplots(1, 3)\n",
        "\n",
        "# Loss for 128\n",
        "ax[0].plot()\n",
        "\n",
        "# Loss for 256\n",
        "\n",
        "# Loss for 512"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJBXCwJekQhj4klSI/wdJb/AMVVNCvQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcO4oJtvGNIX",
        "colab_type": "text"
      },
      "source": [
        "## **3.2 Generator - Generating summaries**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Description:**\n",
        "\n",
        "- Generate summaries\n",
        "- Compute ROUGE metrics on training and validation set\n",
        "- Generate training and validation data for discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFgJV3x24ZJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9b49cdbb-4b2f-4b49-c01f-a98b464f39f3"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# setting\n",
        "rouge = Rouge()\n",
        "def rouge_get_scores(hyp, ref, n):\n",
        "  try:\n",
        "    return float(rouge.get_scores(hyp, ref)[0]['rouge-{}'.format(n)]['f'])\n",
        "  except:\n",
        "    return \"drop\"\n",
        "\n",
        "pad_idx = headline_dictionary.word2index['<pad>']\n",
        "eos_idx = headline_dictionary.word2index['eos']\n",
        "\n",
        "for model_size in model_sizes:\n",
        " # Model specification\n",
        "  grid = {'max_epochs': 25,\n",
        "          'batch_size': 32,\n",
        "          'learning_rate': 3e-4,\n",
        "          'clip': 10,\n",
        "          'l2_reg': 1e-4,\n",
        "          'model_name': \"generator{:.0f}\".format(model_size)\n",
        "        }\n",
        "\n",
        "  ##### model ######\n",
        "  OUTPUT_DIM = len(headline_dictionary.index2word.keys())\n",
        "  ENC_EMB_DIM = pre_train_weight.shape[1]\n",
        "  ENC_HID_DIM = model_size\n",
        "  DEC_HID_DIM = model_size\n",
        "\n",
        "  enc_num_layers = 1 # number of layers in RNN\n",
        "  dec_num_layers = 1 # number of layers in RNN\n",
        "\n",
        "  ENC_DROPOUT = 0.1\n",
        "  DEC_DROPOUT = 0.1\n",
        "\n",
        "  # Initialization\n",
        "  Generator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid['l2_reg'], batch_size = grid['batch_size'],\n",
        "                      text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid['max_epochs'], learning_rate = grid['learning_rate'],\n",
        "                      clip = grid['clip'], teacher_forcing_ratio = 1, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\n",
        "                      DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, enc_num_layers = enc_num_layers, dec_num_layers = dec_num_layers,\n",
        "                      device = device, model_name = grid['model_name'], push_to_repo = push_to_repo)\n",
        "  \n",
        "  # Load model if any\n",
        "  Generator.load()\n",
        "\n",
        "  # Generate summaries for training data and save them\n",
        "  hypotheses = Generator.generate_summaries(text_train, text_lengths_train, headline_train, headline_lengths_train)\n",
        "  hypotheses = sum(\n",
        "      [[' '.join([headline_dictionary.index2word[index] for index in batch[:, hypothesis] if (index != pad_idx) & (index != eos_idx)][1:]) for hypothesis in range(batch.shape[1])] for batch in hypotheses], []\n",
        "  )\n",
        "  references = [' '.join([headline_dictionary.index2word[index] for index in headline_train[:, sentence] if (index != pad_idx) & (index != eos_idx)][1:]) for sentence in range(headline_train.shape[1])]\n",
        "  # trim\n",
        "  lim = Generator.n_batches * grid['batch_size']\n",
        "  references[:lim]\n",
        "\n",
        "  rouge1 = [rouge_get_scores(hyp, ref, '1') for hyp, ref in zip(hypotheses, references)]\n",
        "  rouge1 = np.array([x for x in rouge1 if x != 'drop']).mean()\n",
        "  rouge2 = [rouge_get_scores(hyp, ref, '2') for hyp, ref in zip(hypotheses, references)]\n",
        "  rouge2 = np.array([x for x in rouge2 if x != 'drop']).mean()\n",
        "  rougel = [rouge_get_scores(hyp, ref, 'l') for hyp, ref in zip(hypotheses, references)]\n",
        "  rougel = np.array([x for x in rougel if x != 'drop']).mean()\n",
        "  \n",
        "  # cleaning\n",
        "  del hypotheses, references\n",
        "  gc.collect()\n",
        "\n",
        "  print('Model size = {:.0f}.'.format(model_size))\n",
        "  print('ROUGE-1: {:.3f} on training data.'.format(100*np.array(rouge1)))\n",
        "  print('ROUGE-2: {:.3f} on training data.'.format(100*np.array(rouge2)))\n",
        "  print('ROUGE-l: {:.3f} on training data.'.format(100*np.array(rougel)))\n",
        "  print('---------------')\n",
        "\n",
        "  \n",
        "  ROUGE = {'ROUGE-1': rouge1,\n",
        "           'ROUGE-2': rouge2,\n",
        "           'ROUGE-L': rougel}\n",
        "  # save as json\n",
        "  json_file = json.dumps(ROUGE)\n",
        "  file = open('Results/ROUGE_{:.0f}_train.txt'.format(model_size), \"w\")\n",
        "  file.write(json_file)\n",
        "  file.close()\n",
        "\n",
        "  # Generate summaries for training data and save them\n",
        "  hypotheses = Generator.generate_summaries(text_val, text_lengths_val, headline_val, headline_lengths_val)\n",
        "  hypotheses = sum(\n",
        "      [[' '.join([headline_dictionary.index2word[index] for index in batch[:, hypothesis] if (index != pad_idx) & (index != eos_idx)][1:]) for hypothesis in range(batch.shape[1])] for batch in hypotheses], []\n",
        "  )\n",
        "  references = [' '.join([headline_dictionary.index2word[index] for index in headline_val[:, sentence] if (index != pad_idx) & (index != eos_idx)][1:]) for sentence in range(headline_val.shape[1])]\n",
        "  # trim\n",
        "  n_batches = len(references) // grid['batch_size']\n",
        "  lim = n_batches * grid['batch_size']\n",
        "  references[:lim]\n",
        "\n",
        "  rouge1 = [rouge_get_scores(hyp, ref, '1') for hyp, ref in zip(hypotheses, references)]\n",
        "  rouge1 = np.array([x for x in rouge1 if x != 'drop']).mean()\n",
        "  rouge2 = [rouge_get_scores(hyp, ref, '2') for hyp, ref in zip(hypotheses, references)]\n",
        "  rouge2 = np.array([x for x in rouge2 if x != 'drop']).mean()\n",
        "  rougel = [rouge_get_scores(hyp, ref, 'l') for hyp, ref in zip(hypotheses, references)]\n",
        "  rougel = np.array([x for x in rougel if x != 'drop']).mean()\n",
        "\n",
        "  # cleaning\n",
        "  del hypotheses, references\n",
        "  gc.collect()\n",
        "  \n",
        "  print('Model size = {:.0f}.'.format(model_size))\n",
        "  print('ROUGE-1: {:.3f} on validation data.'.format(100*np.array(rouge1)))\n",
        "  print('ROUGE-2: {:.3f} on validation data.'.format(100*np.array(rouge2)))\n",
        "  print('ROUGE-l: {:.3f} on validation data.'.format(100*np.array(rougel)))\n",
        "  print('---------------')\n",
        "\n",
        "  ROUGE = {'ROUGE-1': rouge1,\n",
        "           'ROUGE-2': rouge2,\n",
        "           'ROUGE-L': rougel}\n",
        "  # save as json\n",
        "  json_file = json.dumps(ROUGE)\n",
        "  file = open('Results/ROUGE_{:.0f}_val.txt'.format(model_size), \"w\")\n",
        "  file.write(json_file)\n",
        "  file.close()\n",
        "\n",
        "# Push everything to github\n",
        "push_to_repo()\n",
        "\"\"\""
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# setting\\nrouge = Rouge()\\ndef rouge_get_scores(hyp, ref, n):\\n  try:\\n    return float(rouge.get_scores(hyp, ref)[0][\\'rouge-{}\\'.format(n)][\\'f\\'])\\n  except:\\n    return \"drop\"\\n\\npad_idx = headline_dictionary.word2index[\\'<pad>\\']\\neos_idx = headline_dictionary.word2index[\\'eos\\']\\n\\nfor model_size in model_sizes:\\n # Model specification\\n  grid = {\\'max_epochs\\': 25,\\n          \\'batch_size\\': 32,\\n          \\'learning_rate\\': 3e-4,\\n          \\'clip\\': 10,\\n          \\'l2_reg\\': 1e-4,\\n          \\'model_name\\': \"generator{:.0f}\".format(model_size)\\n        }\\n\\n  ##### model ######\\n  OUTPUT_DIM = len(headline_dictionary.index2word.keys())\\n  ENC_EMB_DIM = pre_train_weight.shape[1]\\n  ENC_HID_DIM = model_size\\n  DEC_HID_DIM = model_size\\n\\n  enc_num_layers = 1 # number of layers in RNN\\n  dec_num_layers = 1 # number of layers in RNN\\n\\n  ENC_DROPOUT = 0.1\\n  DEC_DROPOUT = 0.1\\n\\n  # Initialization\\n  Generator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid[\\'l2_reg\\'], batch_size = grid[\\'batch_size\\'],\\n                      text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid[\\'max_epochs\\'], learning_rate = grid[\\'learning_rate\\'],\\n                      clip = grid[\\'clip\\'], teacher_forcing_ratio = 1, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\\n                      DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, enc_num_layers = enc_num_layers, dec_num_layers = dec_num_layers,\\n                      device = device, model_name = grid[\\'model_name\\'], push_to_repo = push_to_repo)\\n  \\n  # Load model if any\\n  Generator.load()\\n\\n  # Generate summaries for training data and save them\\n  hypotheses = Generator.generate_summaries(text_train, text_lengths_train, headline_train, headline_lengths_train)\\n  hypotheses = sum(\\n      [[\\' \\'.join([headline_dictionary.index2word[index] for index in batch[:, hypothesis] if (index != pad_idx) & (index != eos_idx)][1:]) for hypothesis in range(batch.shape[1])] for batch in hypotheses], []\\n  )\\n  references = [\\' \\'.join([headline_dictionary.index2word[index] for index in headline_train[:, sentence] if (index != pad_idx) & (index != eos_idx)][1:]) for sentence in range(headline_train.shape[1])]\\n  # trim\\n  lim = Generator.n_batches * grid[\\'batch_size\\']\\n  references[:lim]\\n\\n  rouge1 = [rouge_get_scores(hyp, ref, \\'1\\') for hyp, ref in zip(hypotheses, references)]\\n  rouge1 = np.array([x for x in rouge1 if x != \\'drop\\']).mean()\\n  rouge2 = [rouge_get_scores(hyp, ref, \\'2\\') for hyp, ref in zip(hypotheses, references)]\\n  rouge2 = np.array([x for x in rouge2 if x != \\'drop\\']).mean()\\n  rougel = [rouge_get_scores(hyp, ref, \\'l\\') for hyp, ref in zip(hypotheses, references)]\\n  rougel = np.array([x for x in rougel if x != \\'drop\\']).mean()\\n  \\n  # cleaning\\n  del hypotheses, references\\n  gc.collect()\\n\\n  print(\\'Model size = {:.0f}.\\'.format(model_size))\\n  print(\\'ROUGE-1: {:.3f} on training data.\\'.format(100*np.array(rouge1)))\\n  print(\\'ROUGE-2: {:.3f} on training data.\\'.format(100*np.array(rouge2)))\\n  print(\\'ROUGE-l: {:.3f} on training data.\\'.format(100*np.array(rougel)))\\n  print(\\'---------------\\')\\n\\n  \\n  ROUGE = {\\'ROUGE-1\\': rouge1,\\n           \\'ROUGE-2\\': rouge2,\\n           \\'ROUGE-L\\': rougel}\\n  # save as json\\n  json_file = json.dumps(ROUGE)\\n  file = open(\\'Results/ROUGE_{:.0f}_train.txt\\'.format(model_size), \"w\")\\n  file.write(json_file)\\n  file.close()\\n\\n  # Generate summaries for training data and save them\\n  hypotheses = Generator.generate_summaries(text_val, text_lengths_val, headline_val, headline_lengths_val)\\n  hypotheses = sum(\\n      [[\\' \\'.join([headline_dictionary.index2word[index] for index in batch[:, hypothesis] if (index != pad_idx) & (index != eos_idx)][1:]) for hypothesis in range(batch.shape[1])] for batch in hypotheses], []\\n  )\\n  references = [\\' \\'.join([headline_dictionary.index2word[index] for index in headline_val[:, sentence] if (index != pad_idx) & (index != eos_idx)][1:]) for sentence in range(headline_val.shape[1])]\\n  # trim\\n  n_batches = len(references) // grid[\\'batch_size\\']\\n  lim = n_batches * grid[\\'batch_size\\']\\n  references[:lim]\\n\\n  rouge1 = [rouge_get_scores(hyp, ref, \\'1\\') for hyp, ref in zip(hypotheses, references)]\\n  rouge1 = np.array([x for x in rouge1 if x != \\'drop\\']).mean()\\n  rouge2 = [rouge_get_scores(hyp, ref, \\'2\\') for hyp, ref in zip(hypotheses, references)]\\n  rouge2 = np.array([x for x in rouge2 if x != \\'drop\\']).mean()\\n  rougel = [rouge_get_scores(hyp, ref, \\'l\\') for hyp, ref in zip(hypotheses, references)]\\n  rougel = np.array([x for x in rougel if x != \\'drop\\']).mean()\\n\\n  # cleaning\\n  del hypotheses, references\\n  gc.collect()\\n  \\n  print(\\'Model size = {:.0f}.\\'.format(model_size))\\n  print(\\'ROUGE-1: {:.3f} on validation data.\\'.format(100*np.array(rouge1)))\\n  print(\\'ROUGE-2: {:.3f} on validation data.\\'.format(100*np.array(rouge2)))\\n  print(\\'ROUGE-l: {:.3f} on validation data.\\'.format(100*np.array(rougel)))\\n  print(\\'---------------\\')\\n\\n  ROUGE = {\\'ROUGE-1\\': rouge1,\\n           \\'ROUGE-2\\': rouge2,\\n           \\'ROUGE-L\\': rougel}\\n  # save as json\\n  json_file = json.dumps(ROUGE)\\n  file = open(\\'Results/ROUGE_{:.0f}_val.txt\\'.format(model_size), \"w\")\\n  file.write(json_file)\\n  file.close()\\n\\n# Push everything to github\\npush_to_repo()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t36DDj2zq0Uu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## **3.3 Generator - Generating datases for pretraining of discriminator**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Description:**\n",
        "\n",
        "- We generate summaries as counterparts to real examples given by training and validation set to pretrain our generator.\n",
        "\n",
        "- These summaries are generated using the best generator so far only. (we compare generators based upon ROUGE-1 metrics on validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXCFcYsRTQkP",
        "colab_type": "text"
      },
      "source": [
        "*Function for padding hypothesis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPXzMrBISYq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padded_hypotheses(x, threshold, pad_idx):\n",
        "  \"\"\"\n",
        "  :param x:\n",
        "    type:\n",
        "    description:\n",
        "  :param threshold:\n",
        "    type:\n",
        "    description:\n",
        "  :param pad_idx:\n",
        "    type:\n",
        "    description:\n",
        "\n",
        "  :return x:\n",
        "    type:\n",
        "    description  \n",
        "  \"\"\"\n",
        "  if x.shape[0] == threshold:\n",
        "    return x\n",
        "  else: \n",
        "    return np.r_[x, np.repeat(pad_idx, 32*(threshold - x.shape[0])).reshape(-1, 32)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDN2t6pYq2h3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5fb4f5ed-a233-4e65-d0f7-985078d580b2"
      },
      "source": [
        "best_model_size = model_sizes[np.array([float(open('Results/ROUGE_{:.0f}_val.txt'.format(model_size), \"r\").read().split(', ')[0].split(': ')[1]) for model_size in model_sizes]).argmax()]\n",
        "\n",
        "print(\"The model with hidden size of {:.0f} is the best performing model w.r.t. ROUGE-1 metric on validation set.\".format(best_model_size))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model with hidden size of 512 is the best performing model w.r.t. ROUGE-1 metric on validation set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97kkwn8T85jQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "892a117b-a659-4406-8e0c-61786e2ff4bb"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "# Model specification\n",
        "grid = {'max_epochs': 25,\n",
        "        'batch_size': 32,\n",
        "        'learning_rate': 3e-4,\n",
        "        'clip': 10,\n",
        "        'l2_reg': 1e-4,\n",
        "        'model_name': \"generator{:.0f}\".format(best_model_size)\n",
        "      }\n",
        "\n",
        "##### model ######\n",
        "OUTPUT_DIM = len(headline_dictionary.index2word.keys())\n",
        "ENC_EMB_DIM = pre_train_weight.shape[1]\n",
        "ENC_HID_DIM = best_model_size\n",
        "DEC_HID_DIM = best_model_size\n",
        "\n",
        "enc_num_layers = 1 # number of layers in RNN\n",
        "dec_num_layers = 1 # number of layers in RNN\n",
        "\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "# Initialization\n",
        "Generator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid['l2_reg'], batch_size = grid['batch_size'],\n",
        "                    text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid['max_epochs'], learning_rate = grid['learning_rate'],\n",
        "                    clip = grid['clip'], teacher_forcing_ratio = 1, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\n",
        "                    DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, enc_num_layers = enc_num_layers, dec_num_layers = dec_num_layers,\n",
        "                    device = device, model_name = grid['model_name'], push_to_repo = push_to_repo)\n",
        "\n",
        "# Load model if any\n",
        "Generator.load()\n",
        "\n",
        "# Generate summaries for training data\n",
        "hypotheses = Generator.generate_summaries(text_train, text_lengths_train, headline_train, headline_lengths_train)\n",
        "# Pad hypotheses\n",
        "hypotheses = np.concatenate(\n",
        "    [padded_hypotheses(hypothesis, 68, headline_dictionary.word2index['<pad>']) for hypothesis in hypotheses], axis = 1\n",
        ")\n",
        "# Correct the 'sos' symbol\n",
        "hypotheses[0, :] = 0\n",
        "# Concatenate real and fake summaries + transpose\n",
        "real_fake_train = np.concatenate((headline_train, hypotheses), axis = 1)\n",
        "real_fake_train = np.swapaxes(real_fake_train, 0, 1) # shape [n_examples, seq_len]\n",
        "# add labels as the first column - 1 = Real, 0 = Generated\n",
        "real_fake_train = np.c_[np.vstack((np.ones((headline_train.shape[1], 1)), np.zeros((hypotheses.shape[1], 1)))), real_fake_train]\n",
        "# save\n",
        "np.save('../data/real_fake_train.npy', real_fake_train)\n",
        "del hypotheses\n",
        "\n",
        "# Generate summaries for validation data\n",
        "hypotheses = Generator.generate_summaries(text_val, text_lengths_val, headline_val, headline_lengths_val)\n",
        "# Pad hypotheses\n",
        "hypotheses = np.concatenate(\n",
        "    [padded_hypotheses(hypothesis, headline_val.shape[0], headline_dictionary.word2index['<pad>']) for hypothesis in hypotheses], axis = 1\n",
        ")\n",
        "# Correct the 'sos' symbol\n",
        "hypotheses[0, :] = 0\n",
        "# Concatenate real and fake summaries + transpose\n",
        "real_fake_val = np.concatenate((headline_val, hypotheses), axis = 1)\n",
        "real_fake_val = np.swapaxes(real_fake_train, 0, 1) # shape [n_examples, seq_len]\n",
        "# add labels as the first column - 1 = Real, 0 = Generated\n",
        "real_fake_val = np.c_[np.vstack((np.ones((headline_val.shape[1], 1)), np.zeros((hypotheses.shape[1], 1)))), real_fake_val]\n",
        "# reshuffle\n",
        "np.random.shuffle(real_fake_val)\n",
        "# save\n",
        "np.save('../data/real_fake_val.npy', real_fake_val)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# Model specification\\ngrid = {\\'max_epochs\\': 25,\\n        \\'batch_size\\': 32,\\n        \\'learning_rate\\': 3e-4,\\n        \\'clip\\': 10,\\n        \\'l2_reg\\': 1e-4,\\n        \\'model_name\\': \"generator{:.0f}\".format(best_model_size)\\n      }\\n\\n##### model ######\\nOUTPUT_DIM = len(headline_dictionary.index2word.keys())\\nENC_EMB_DIM = pre_train_weight.shape[1]\\nENC_HID_DIM = best_model_size\\nDEC_HID_DIM = best_model_size\\n\\nenc_num_layers = 1 # number of layers in RNN\\ndec_num_layers = 1 # number of layers in RNN\\n\\nENC_DROPOUT = 0.1\\nDEC_DROPOUT = 0.1\\n\\n# Initialization\\nGenerator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid[\\'l2_reg\\'], batch_size = grid[\\'batch_size\\'],\\n                    text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid[\\'max_epochs\\'], learning_rate = grid[\\'learning_rate\\'],\\n                    clip = grid[\\'clip\\'], teacher_forcing_ratio = 1, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\\n                    DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, enc_num_layers = enc_num_layers, dec_num_layers = dec_num_layers,\\n                    device = device, model_name = grid[\\'model_name\\'], push_to_repo = push_to_repo)\\n\\n# Load model if any\\nGenerator.load()\\n\\n# Generate summaries for training data\\nhypotheses = Generator.generate_summaries(text_train, text_lengths_train, headline_train, headline_lengths_train)\\n# Pad hypotheses\\nhypotheses = np.concatenate(\\n    [padded_hypotheses(hypothesis, 68, headline_dictionary.word2index[\\'<pad>\\']) for hypothesis in hypotheses], axis = 1\\n)\\n# Correct the \\'sos\\' symbol\\nhypotheses[0, :] = 0\\n# Concatenate real and fake summaries + transpose\\nreal_fake_train = np.concatenate((headline_train, hypotheses), axis = 1)\\nreal_fake_train = np.swapaxes(real_fake_train, 0, 1) # shape [n_examples, seq_len]\\n# add labels as the first column - 1 = Real, 0 = Generated\\nreal_fake_train = np.c_[np.vstack((np.ones((headline_train.shape[1], 1)), np.zeros((hypotheses.shape[1], 1)))), real_fake_train]\\n# save\\nnp.save(\\'../data/real_fake_train.npy\\', real_fake_train)\\ndel hypotheses\\n\\n# Generate summaries for validation data\\nhypotheses = Generator.generate_summaries(text_val, text_lengths_val, headline_val, headline_lengths_val)\\n# Pad hypotheses\\nhypotheses = np.concatenate(\\n    [padded_hypotheses(hypothesis, headline_val.shape[0], headline_dictionary.word2index[\\'<pad>\\']) for hypothesis in hypotheses], axis = 1\\n)\\n# Correct the \\'sos\\' symbol\\nhypotheses[0, :] = 0\\n# Concatenate real and fake summaries + transpose\\nreal_fake_val = np.concatenate((headline_val, hypotheses), axis = 1)\\nreal_fake_val = np.swapaxes(real_fake_train, 0, 1) # shape [n_examples, seq_len]\\n# add labels as the first column - 1 = Real, 0 = Generated\\nreal_fake_val = np.c_[np.vstack((np.ones((headline_val.shape[1], 1)), np.zeros((hypotheses.shape[1], 1)))), real_fake_val]\\n# reshuffle\\nnp.random.shuffle(real_fake_val)\\n# save\\nnp.save(\\'../data/real_fake_val.npy\\', real_fake_val)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObSQ_j-nkiaD",
        "colab_type": "text"
      },
      "source": [
        "## **3.4 Discriminator - Pretraining**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Description:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl3CRg-Ilsme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data\n",
        "real_fake_train = np.load('../data/real_fake_train.npy', allow_pickle = False)\n",
        "real_fake_val = np.load('../data/real_fake_val.npy', allow_pickle = False)\n",
        "\n",
        "# Split into X and y\n",
        "X_train, y_train = torch.from_numpy(real_fake_train[:, 1:]).long(), torch.from_numpy(real_fake_train[:, 0]).long()\n",
        "X_val, y_val = torch.from_numpy(real_fake_val[:, 1:]).long(), torch.from_numpy(real_fake_val[:, 0]).long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKMQfxgqaSQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "5ef136ed-b930-4087-8959-15362a06149f"
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "for n_kernels in [10, 20, 30, 50]:\n",
        "  for dropout in [0.0, 0.2, 0.3, 0.5]:\n",
        "    param = {'max_epochs': 80,\n",
        "            'learning_rate': 5e-4,\n",
        "            'batch_size': 32,               \n",
        "            'seq_len': 68,                   # length of your summary\n",
        "            'embed_dim': 200,\n",
        "            'drop_out': dropout,\n",
        "            'kernel_num': n_kernels,                 # number of your feature map\n",
        "            'in_channel': 1,                 # for text classification should be one\n",
        "            # how many conv net are used in parallel in text classification\n",
        "            'parallel_layer': 3,\n",
        "            'model_name': 'n_{:.0f}_d_{:.0f}'.format(n_kernels, 10*dropout),\n",
        "            'device':'cuda'}\n",
        "    print('----------')\n",
        "    print(f'Kernel filters = {n_kernels:.0f}, Dropout prob. = {dropout:.1f}')\n",
        "    drt = Discriminator_utility(pre_train_weight_head,**param)\n",
        "    drt.run_epochs(X_train,y_train,X_test = X_val, y_test = y_val)\n",
        "    push_to_repo()\n",
        "    # print accuracy on the validation data\n",
        "    print(\"Kernel filters: {:.0f}, dropout: {:.1f} => Accuracy: {:.2f} %.\".format(n_kernels, dropout, 100*drt.predict(X_val, y_val)))\n",
        "    print('----------')\n",
        "    if min(drt.val_losses) < best_val_loss:\n",
        "      best_val_loss = min(drt.val_losses)\n",
        "      best_n_kernels, best_dropout = n_kernels, dropout\n",
        "\n",
        "print(f'The best performing model has {best_n_kernels:.0f} with drop. prob. {best_dropout:.1f} and performin loss of {best_val_loss:.3f} on validation data.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "Kernel filters = 10, Dropout prob. = 0.0\n",
            "Epoch: 1:\n",
            "Train Loss: 0.219\n",
            "Validation Loss: 0.176\n",
            "Epoch: 2:\n",
            "Train Loss: 0.154\n",
            "Validation Loss: 0.157\n",
            "Epoch: 3:\n",
            "Train Loss: 0.140\n",
            "Validation Loss: 0.151\n",
            "Epoch: 4:\n",
            "Train Loss: 0.131\n",
            "Validation Loss: 0.148\n",
            "Epoch: 5:\n",
            "Train Loss: 0.126\n",
            "Validation Loss: 0.143\n",
            "Epoch: 6:\n",
            "Train Loss: 0.121\n",
            "Validation Loss: 0.140\n",
            "Epoch: 7:\n",
            "Train Loss: 0.118\n",
            "Validation Loss: 0.136\n",
            "Epoch: 8:\n",
            "Train Loss: 0.115\n",
            "Validation Loss: 0.133\n",
            "Epoch: 9:\n",
            "Train Loss: 0.113\n",
            "Validation Loss: 0.133\n",
            "Epoch: 10:\n",
            "Train Loss: 0.111\n",
            "Validation Loss: 0.130\n",
            "Epoch: 11:\n",
            "Train Loss: 0.109\n",
            "Validation Loss: 0.130\n",
            "Epoch: 12:\n",
            "Train Loss: 0.108\n",
            "Validation Loss: 0.128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pDTi7rPQpDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the training class (generator)\n",
        "run Code/Models/discriminator_training_class.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cmkydK4nVoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmEbwSMomDaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}