{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dixKtNdJhriq",
        "colab_type": "text"
      },
      "source": [
        "# **Headline Generation via Adversarial Training**\n",
        "## **Project for Statistical Natural Language Processing (COMP0087)**\n",
        "## **University College London**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**File: Data Preprocessing.ipynb**\n",
        "\n",
        "**Collaborators:**\n",
        "  - Daniel Stancl (ucabds7@ucl.ac.uk)\n",
        "  - Guoliang HE (ucabggh@ucl.ac.uk)\n",
        "  - Dorota Jagnesakova (ucabdj1@ucl.ac.uk)\n",
        "  - Zakhar Borok (zcabzbo@ucl.ac.uk)\n",
        "\n",
        "<hr>\n",
        "\n",
        "### **Description:** Colab notebook which downloads WikiHow data and GloVe embeddings. Then, input data are preprocessed, GloVe embeddings is trimmed/filtered to the words which are present in the training data. Eventually, input data are transformed to continuous representation using this pre-trained embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoJ3uJ4kiWlQ",
        "colab_type": "text"
      },
      "source": [
        "# **1 Setup**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- set GitHub credentials, clone repository, and define helper *push* function\n",
        "- install and import all required libraries\n",
        "- run auxiliary python scripts\n",
        "- download pre-traind embeddings (GloVe)\n",
        "- define function for filtering only those embedding vectors which are needed for our training data\n",
        "- load the WikiHow data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-PdoqujiaW_",
        "colab_type": "text"
      },
      "source": [
        "## **1.1 GitHub stuff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBwA3aeAilFH",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.1 Set GitHub credentials and username of repo owner**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ay0We_0hj3G",
        "colab_type": "code",
        "outputId": "3d33b9d7-d8b1-475e-84d2-e0747f70d031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# credentials\n",
        "user_email = '<your_email>'\n",
        "user = '<your_username>'\n",
        "user_password = \"<your_password>\"\n",
        "\n",
        "# username of repo owner\n",
        "owner_username = 'stancld'\n",
        "# reponame\n",
        "reponame = 'GeneratingHeadline_GANs'\n",
        "\n",
        "# generate \n",
        "add_origin_link = (\n",
        "    'https://{}:{}github@github.com/{}/{}.git'.format(\n",
        "    user, user_password, owner_username, reponame)\n",
        ")\n",
        "\n",
        "print(\"Link used for git cooperation:\\n{}\".format(add_origin_link))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Link used for git cooperation:\n",
            "https://<your_username>:<your_password>github@github.com/stancld/GeneratingHeadline_GANs.git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnAeQe-Pi5OP",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.2 Clone GitHub repo on the personal drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHJuTxPci46_",
        "colab_type": "code",
        "outputId": "70e49d64-9385-4531-9866-0e2fb632307a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "## Clone GitHub repo to the desired folder\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "%cd \"drive/My Drive/projects\"\n",
        "\n",
        "# Remove NLP_Project if presented and clone up-to-date repo\n",
        "!rm -r GeneratingHeadline_GANs\n",
        "!git clone https://github.com/stancld/GeneratingHeadline_GANs.git\n",
        "\n",
        "# Go to the NLP_Project folder\n",
        "%cd GeneratingHeadline_GANs\n",
        "\n",
        "# Config global user and add origin enabling us to execute push commands\n",
        "!git config --global user.email user_email\n",
        "!git remote rm origin\n",
        "!git remote add origin https://<your_username>:<your_password>@github.com/stancld/GeneratingHeadline_GANs.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n%%time\\n\\n## Clone GitHub repo to the desired folder\\n# Mount drive\\nfrom google.colab import drive\\ndrive.mount(\"/content/drive\", force_remount = True)\\n%cd \"drive/My Drive/projects\"\\n\\n# Remove NLP_Project if presented and clone up-to-date repo\\n!rm -r GeneratingHeadline_GANs\\n!git clone https://github.com/stancld/GeneratingHeadline_GANs.git\\n\\n# Go to the NLP_Project folder\\n%cd GeneratingHeadline_GANs\\n\\n# Config global user and add origin enabling us to execute push commands\\n!git config --global user.email user_email\\n!git remote rm origin\\n!git remote add origin https://<your_username>:<your_password>@github.com/stancld/GeneratingHeadline_GANs.git\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q46T4fRjKWX",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.3 Helper function: push_to_repo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFxVTx2VjKpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def push_to_repo():\n",
        "  \"\"\"\n",
        "  Helper function that pushes saved fils to github repo.\n",
        "  \"\"\"\n",
        "  !git remote rm origin\n",
        "  !git remote add origin https://<your_username>:<your_password>@github.com/stancld/GeneratingHeadline_GANs.git\n",
        "  !git checkout master\n",
        "  !git pull origin master\n",
        "  !git checkout models_branch\n",
        "  !git add .\n",
        "  !git commit -m \"model state update\"\n",
        "  !git checkout master\n",
        "  !git merge models_branch\n",
        "  !git push -u origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHA62OOqjKRd",
        "colab_type": "text"
      },
      "source": [
        "## **1.2 General stuff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H5CsJUHllW3",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.1 Install and import packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrocbitplhJR",
        "colab_type": "code",
        "outputId": "d74702bb-76d4-498b-daac-982719bf10a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "pip install rouge==1.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge==1.0.0) (1.12.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skBOwJhulq0d",
        "colab_type": "code",
        "outputId": "3fccc468-8193-4fa6-b916-72bf085f6070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import copy\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from rouge import Rouge\n",
        "from termcolor import colored\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJUn6pJOlucH",
        "colab_type": "code",
        "outputId": "dd19b64c-0220-44cc-99df-795a9abf6b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAhy3doxl6uO",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.2 Run auxiliary Python scripts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc-jDg_Sl5V3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Contractions\n",
        "run Code/contractions.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiFt7v4GmIIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for text_preprocessing()\n",
        "run Code/text_preprocessing.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srHNXua_mIki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for transforming data to padded array\n",
        "run Code/data2PaddedArray.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "minlJwgdmH6z",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.3 Download pre-trained embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUTH2_r2mSdo",
        "colab_type": "code",
        "outputId": "b1cb8c05-3df5-40c4-9764-10826b595b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "# Set desired dimension of embeddings from [50, 100, 200, 300]\n",
        "embed_dim = 200\n",
        "\n",
        "# Download and unzip GloVe embedding\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# input your pre-train txt path and parse the data\n",
        "path = '../data/glove.6B.{:.0f}d.txt'.format(embed_dim)\n",
        "\n",
        "embed_dict = {}\n",
        "with open(path,'r') as f:\n",
        "  lines = f.readlines()\n",
        "  for l in lines:\n",
        "    w = l.split()[0]\n",
        "    v = np.array(l.split()[1:]).astype('float')\n",
        "    embed_dict[w] = v\n",
        "\n",
        "embed_dict['@@_unknown_@@'] = np.random.random(embed_dim)\n",
        "\n",
        "# remove all the unnecesary files\n",
        "!rm -rf glove.6B.zip\n",
        "!rm -rf glove.6B.50d.txt\n",
        "!rm -rf glove.6B.100d.txt\n",
        "!rm -rf glove.6B.200d.txt\n",
        "!rm -rf glove.6B.300d.txt\n",
        "\n",
        "# check the length of the dictionary\n",
        "len(embed_dict.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-27 08:09:27--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-03-27 08:09:27--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-03-27 08:09:27--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.01MB/s    in 6m 28s  \n",
            "\n",
            "2020-03-27 08:15:56 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyH3yq62mvd3",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.4 Helper function extracting only those embeddings vectors which are presented in our training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8704Fzjmvy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_weight(text_dictionary):\n",
        "  \"\"\"\n",
        "  Helper function extracting only those embeddings vectors which are presented in our training data\n",
        "  \n",
        "  :param text_dictionary:\n",
        "    type: Dictionary\n",
        "    description: Pre-trained embeddings\n",
        "\n",
        "  :return pre_train_weight:\n",
        "    type: Dictionary\n",
        "    description: Filtered pre-trained embeddings\n",
        "  \"\"\"\n",
        "  pre_train_weight = []\n",
        "  for word_index in text_dictionary.index2word.keys():\n",
        "    if word_index != 0:\n",
        "      word = text_dictionary.index2word[word_index]\n",
        "      try:\n",
        "        word_vector = embed_dict[word].reshape(1,-1)\n",
        "      except:\n",
        "        word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word\n",
        "      pre_train_weight = np.vstack([pre_train_weight,word_vector])\n",
        "    \n",
        "    # add for padding\n",
        "    elif word_index == len(text_dictionary.index2word.keys()):  \n",
        "      pre_train_weight = np.r_[pre_train_weight, np.zeros((1, embed_dim))]\n",
        "    \n",
        "    else:\n",
        "      word = text_dictionary.index2word[word_index]\n",
        "      try:\n",
        "        word_vector = embed_dict[word].reshape(1,-1)\n",
        "      except:\n",
        "        word_vector = embed_dict['@@_unknown_@@'].reshape(1,-1) # handle unknown word\n",
        "      pre_train_weight = word_vector\n",
        "  return pre_train_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FVkk-Yzn54f",
        "colab_type": "text"
      },
      "source": [
        "## **1.3 Load the data**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Source:**  https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzENwdfEn5o2",
        "colab_type": "code",
        "outputId": "63a4a2f7-27d4-4334-d3a0-d773c3ab801e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%%time\n",
        "# Open\n",
        "data = pd.read_csv('../data/wikihowSep.csv',\n",
        "                    error_bad_lines = False).astype(str)\n",
        "print(data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1585695, 5)\n",
            "CPU times: user 13.2 s, sys: 1.32 s, total: 14.5 s\n",
            "Wall time: 17.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50XWT9f2nRH-",
        "colab_type": "text"
      },
      "source": [
        "# **2 Preprocess the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPOIN9E9olcF",
        "colab_type": "text"
      },
      "source": [
        "## **2.1 Text Preprocessing using our predefined function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86sDQ1WynUco",
        "colab_type": "code",
        "outputId": "f6d673c8-b228-477a-a06e-2e2bf21d89be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "# Preprocess\n",
        "for item in ['text', 'headline']:\n",
        "  exec(\"\"\"{}_data = text_preprocessing(data=data,\n",
        "    item = '{}',\n",
        "    contraction_map=CONTRACTION_MAP,\n",
        "    drop_digits=False,\n",
        "    remove_stopwords=False,\n",
        "    stemming=False)\"\"\".format(item, item), locals(), globals()\n",
        ")\n",
        "\n",
        "# Cleaning\n",
        "del data\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 14s, sys: 8.49 s, total: 4min 22s\n",
            "Wall time: 4min 22s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sijp_ncdnyWn",
        "colab_type": "text"
      },
      "source": [
        "## **2.2 Clean the data according to the rules specified within the report**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpMJTEwXrp98",
        "colab_type": "text"
      },
      "source": [
        "**Print some statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTCf0L8npDTC",
        "colab_type": "code",
        "outputId": "2a81309a-0ec3-4f55-d140-272c2b59748f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# get lengths of input articles and target summaries\n",
        "text_len = [len(t) for t in text_data]\n",
        "head_len = [len(h) for h in headline_data]\n",
        "\n",
        "# Print some statistics of the uncleansed data\n",
        "print('Some statistics')\n",
        "\n",
        "print('Average length of articles is {:.2f}.'.format(np.array(text_len).mean()))\n",
        "print('Min = {:.0f}, Max = {:.0f}, Std = {:.2f}'.format(min(text_len), max(text_len), np.array(text_len).std()))\n",
        "\n",
        "print('-----')\n",
        "\n",
        "print('Average length of summaries is {:.2f}.'.format(np.array(head_len).mean()))\n",
        "print('Min = {:.0f}, Max = {:.0f}, Std = {:.2f}'.format(min(head_len), max(head_len), np.array(head_len).std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some statistics\n",
            "Average length of articles is 65.62.\n",
            "Min = 2, Max = 2967, Std = 58.83\n",
            "-----\n",
            "Average length of summaries is 11.08.\n",
            "Min = 2, Max = 2945, Std = 6.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYoscg3FpKiI",
        "colab_type": "code",
        "outputId": "945f0092-c3d2-41f1-9a4c-31d384f16e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# specified the maximum number of examples and the maximum threshold of lentghts of input articles and target summaries\n",
        "max_examples = 150000\n",
        "max_threshold = 0.75\n",
        "\n",
        "# drop examples with an invalid ratio of length of text and headline\n",
        "text_len = [len(t) for t in text_data]\n",
        "head_len = [len(h) for h in headline_data]\n",
        "\n",
        "ratio = [h/t for t, h in zip(text_len, head_len)]\n",
        "\n",
        "problems1 = [problem for problem, r in enumerate(ratio) if (r > max_threshold)]\n",
        "print(len(problems1))\n",
        "text_data, headline_data = np.delete(text_data, problems1), np.delete(headline_data, problems1)\n",
        "print(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\n",
        "\n",
        "# drop too long articles (to avoid struggles with CUDA memory) and too short\n",
        "text_len = [len(t) for t in text_data]\n",
        "\n",
        "problems2 = [problem for problem, text_length in enumerate(text_len) if ((text_length > 200) | (text_length < 10) )]\n",
        "print(len(problems2))\n",
        "text_data, headline_data = np.delete(text_data, problems2), np.delete(headline_data, problems2)\n",
        "print(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\n",
        "\n",
        "# drop too pairs with too short/long summaries\n",
        "head_len = [len(h) for h in headline_data]\n",
        "\n",
        "problems3 = [problem for problem, headline_len in enumerate(head_len) if ( (headline_len > 75) | (headline_len < 2) )]\n",
        "print(len(problems3))\n",
        "text_data, headline_data = np.delete(text_data, problems3), np.delete(headline_data, problems3)\n",
        "print(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\n",
        "\n",
        "# some cleaning\n",
        "del text_len, head_len, ratio, problems1, problems2, problems3\n",
        "gc.collect()\n",
        "\n",
        "# trim the data to have only a subset of the data for our project\n",
        "try:\n",
        "  text_data, headline_data = text_data[:max_examples], headline_data[:max_examples]\n",
        "except:\n",
        "  pass\n",
        "\n",
        "print(text_data.shape, headline_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "326422\n",
            "Number of examples after filtering: 1259273\n",
            "44706\n",
            "Number of examples after filtering: 1214567\n",
            "32\n",
            "Number of examples after filtering: 1214535\n",
            "(150000,) (150000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au_w1igorsxN",
        "colab_type": "text"
      },
      "source": [
        "**Print some statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6j9fSeqrnBF",
        "colab_type": "code",
        "outputId": "005d622a-e6cd-41a0-d183-1220881dce0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# get lengths of input articles and target summaries\n",
        "text_len = [len(t) for t in text_data]\n",
        "head_len = [len(h) for h in headline_data]\n",
        "\n",
        "print('Some statistics')\n",
        "\n",
        "print('Average length of articles is {:.2f}.'.format(np.array(text_len).mean()))\n",
        "print('Min = {:.0f}, Max = {:.0f}, Std = {:.2f}'.format(min(text_len), max(text_len), np.array(text_len).std()))\n",
        "\n",
        "print('-----')\n",
        "\n",
        "print('Average length of summaries is {:.2f}.'.format(np.array(head_len).mean()))\n",
        "print('Min = {:.0f}, Max = {:.0f}, Std = {:.2f}'.format(min(head_len), max(head_len), np.array(head_len).std()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some statistics\n",
            "Average length of articles is 87.47.\n",
            "Min = 10, Max = 200, Std = 42.66\n",
            "-----\n",
            "Average length of summaries is 9.45.\n",
            "Min = 3, Max = 68, Std = 4.49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qODLoXBGpvW9",
        "colab_type": "text"
      },
      "source": [
        "## **2.3 Split the data**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- Trainin data - ~90%\n",
        "- Validation/dev data - ~5%\n",
        "- Test data - ~5%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSPBn87gpxhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(222)\n",
        "\n",
        "split = np.random.uniform(0, 1, size = text_data.shape[0])\n",
        "\n",
        "# Train set\n",
        "text_train, headline_train = text_data[split <= 0.9], headline_data[split <= 0.9]\n",
        "# Validation set\n",
        "text_val, headline_val = text_data[(split > 0.9) & (split <= 0.95)], headline_data[(split > 0.9) & (split <= 0.95)]\n",
        "# Test set\n",
        "text_test, headline_test = text_data[split > 0.95], headline_data[split > 0.95]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7gp5238tD_A",
        "colab_type": "text"
      },
      "source": [
        "**Print some statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJnWHKICtEUk",
        "colab_type": "code",
        "outputId": "7c5016c8-a72a-497d-d25d-a6823b3ab2b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('Average lengths of articles is {:.2f}'.format(np.array([len(text) for text in text_train]).mean()))\n",
        "\n",
        "print('Average lengths of sumaries is {:.2f}'.format(np.array([len(text) for text in headline_train]).mean()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average lengths of articles is 87.39\n",
            "Average lengths of sumaries is 9.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UZrCf5-tZGf",
        "colab_type": "text"
      },
      "source": [
        "## **2.4 Sort the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oumR-Buptkc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_data(text, headline):\n",
        "  \"\"\"\n",
        "  Function sorting data w.r.t. to the lengths of input articles, from the longest to the shortes one\n",
        "\n",
        "  :param text:\n",
        "    type: Numpy.Object\n",
        "    description: Input articles\n",
        "  :param headline:\n",
        "    type: Numpy.Object\n",
        "    descritpion: Target summaries\n",
        "\n",
        "  :return text:  \n",
        "    type: Numpy.Object\n",
        "    description: Sorted input articles from the longest one to the shortest one\n",
        "  :return headline:\n",
        "    type: Numpy.Object\n",
        "    description: Rearranged target summaries w.r.t. input articles\n",
        "  \"\"\"\n",
        "  headline = np.array(\n",
        "      [y for x,y in sorted(zip(text, headline), key = lambda pair: len(pair[0]), reverse = True)]\n",
        "  )\n",
        "  text = list(text)\n",
        "  text.sort(key = lambda x: len(x), reverse = True)\n",
        "  text = np.array(text)\n",
        "\n",
        "  return text, headline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN4aX1h3tYx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train set\n",
        "text_train, headline_train = sort_data(text_train, headline_train)\n",
        "# Validation set\n",
        "text_val, headline_val = sort_data(text_val, headline_val)\n",
        "# Test set\n",
        "text_test, headline_test = sort_data(text_test, headline_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA25jIcFufW9",
        "colab_type": "text"
      },
      "source": [
        "# **3 Text & Headline dictionary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXJ7HCfUzA8j",
        "colab_type": "text"
      },
      "source": [
        "## **3.1 Extract dictionaries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6w7ylJTujze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class Language Dictionary\n",
        "class LangDict:\n",
        "  \"\"\"\n",
        "  Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "    self.n_words = 2\n",
        "\n",
        "  def add_article(self, article):\n",
        "    for word in article:\n",
        "      self.add_word(word)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOO-VuM-y2Jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dictionary based on the training data\n",
        "text_dictionary = LangDict()\n",
        "headline_dictionary = LangDict()\n",
        "\n",
        "for article in text_train:\n",
        "  text_dictionary.add_article(article)\n",
        "\n",
        "for article in headline_train:\n",
        "  headline_dictionary.add_article(article)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO6nwUYEzFSb",
        "colab_type": "text"
      },
      "source": [
        "## **3.2 Print some statistics and drop infrequent words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhdxSm44zETb",
        "colab_type": "code",
        "outputId": "5d39bc77-dd32-4dbc-dc40-8f4919e85375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(\"There are {:.0f} distinct words in the untrimmed text dictionary\".format(len(text_dictionary.word2index.keys())))\n",
        "print(\"There are {:.0f} distinct words in the untrimmed headline dictionary\".format(len(headline_dictionary.word2index.keys())))\n",
        "\n",
        "# Trim a dictionary to the words with at least 10 occurences within the text\n",
        "text_min_count = 1\n",
        "head_min_count = 2\n",
        "\n",
        "## TEXT DICTIONARY\n",
        "subset_words = [word for (word, count) in text_dictionary.word2count.items() if count >= text_min_count]\n",
        "text_dictionary.word2index = {word: i for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "text_dictionary.index2word = {i: word for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "text_dictionary.word2count = {word: count for (word, count) in text_dictionary.word2count.items() if count >= text_min_count}\n",
        "\n",
        "## HEADLINE DICTIONARY\n",
        "subset_words = [word for (word, count) in headline_dictionary.word2count.items() if count >= head_min_count]\n",
        "headline_dictionary.word2index = {word: i for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "headline_dictionary.index2word = {i: word for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "headline_dictionary.word2count = {word: count for (word, count) in headline_dictionary.word2count.items() if count >= head_min_count}\n",
        "\n",
        "print(\"There are {:.0f} distinct words in the trimmed text dictionary, where only word with at least {:.0f} occurences are retained\".format(len(text_dictionary.word2index.keys()), text_min_count))\n",
        "print(\"There are {:.0f} distinct words in the trimmed headline dictionary, where only word with at least {:.0f} occurences are retained\".format(len(headline_dictionary.word2index.keys()), head_min_count))\n",
        "del text_min_count, head_min_count, subset_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 67860 distinct words in the untrimmed text dictionary\n",
            "There are 23368 distinct words in the untrimmed headline dictionary\n",
            "There are 67860 distinct words in the trimmed text dictionary, where only word with at least 1 occurences are retained\n",
            "There are 15049 distinct words in the trimmed headline dictionary, where only word with at least 2 occurences are retained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGCnPNpyzeSo",
        "colab_type": "text"
      },
      "source": [
        "## **3.3 Add pad token**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clGXtD9ozd7z",
        "colab_type": "code",
        "outputId": "0d584a99-baf7-48bc-94cd-c1e055f941c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "## TEXT DICTIONARY\n",
        "pad_idx = max(list(text_dictionary.index2word.keys())) + 1\n",
        "\n",
        "text_dictionary.word2index['<pad>'] = pad_idx\n",
        "text_dictionary.index2word[pad_idx] = '<pad>'\n",
        "\n",
        "print(len(text_dictionary.index2word.keys()))\n",
        "\n",
        "## HEADLINE DICTIONARY\n",
        "pad_idx = max(list(headline_dictionary.index2word.keys())) + 1\n",
        "\n",
        "headline_dictionary.word2index['<pad>'] = pad_idx\n",
        "headline_dictionary.index2word[pad_idx] = '<pad>'\n",
        "\n",
        "print(len(headline_dictionary.index2word.keys()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "67861\n",
            "15050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyVrp_oZ1mEq",
        "colab_type": "text"
      },
      "source": [
        "## **3.4 Save dictionaries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDSM5AbE1l4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_dictionary\n",
        "with open('../data/text.dictionary', 'wb') as text_dictionary_file:\n",
        "  pickle.dump(text_dictionary, text_dictionary_file)\n",
        "\n",
        "# headline_dictionary\n",
        "with open('../data/headline.dictionary', 'wb') as headline_dictionary_file:\n",
        "  pickle.dump(headline_dictionary, headline_dictionary_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VibIevO30hP3",
        "colab_type": "text"
      },
      "source": [
        "## **3.5 Extract embedding vectors and save them**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4aJbfBq0grf",
        "colab_type": "code",
        "outputId": "e2f1a444-f755-4e14-db35-f33c63c7d1f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "pre_train_weight = extract_weight(text_dictionary)\n",
        "pre_train_weight = np.array(pre_train_weight, dtype = np.float32)\n",
        "np.save('../data/embedding.npy', pre_train_weight)\n",
        "\n",
        "pre_train_weight_head = extract_weight(headline_dictionary)\n",
        "pre_train_weight_head = np.array(pre_train_weight_head, dtype = np.float32)\n",
        "np.save('../data/embedding_headline.npy', pre_train_weight_head)\n",
        "\n",
        "del embed_dict\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 15min 33s, sys: 31.3 s, total: 16min 4s\n",
            "Wall time: 16min 9s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tMqWmmw1O-U",
        "colab_type": "text"
      },
      "source": [
        "# **4. Transform the data into the sequence of indexed words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfEFpBSL1ObP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train set\n",
        "text_train, text_lengths_train, headline_train, headline_lengths_train = data2PaddedArray(text_train, headline_train, {'text_dictionary': text_dictionary,\n",
        "                                                                                                                       'headline_dictionary': headline_dictionary},\n",
        "                                                                                          pre_train_weight)\n",
        "\n",
        "# Validation set\n",
        "text_val, text_lengths_val, headline_val, headline_lengths_val = data2PaddedArray(text_val, headline_val, {'text_dictionary': text_dictionary,\n",
        "                                                                                                           'headline_dictionary': headline_dictionary},\n",
        "                                                                                  pre_train_weight)\n",
        "\n",
        "# Test set\n",
        "text_test, text_lengths_test, headline_test, headline_lengths_test = data2PaddedArray(text_test, headline_test, {'text_dictionary': text_dictionary,\n",
        "                                                                                                                 'headline_dictionary': headline_dictionary},\n",
        "                                                                                       pre_train_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "487jmrKO1ZE2",
        "colab_type": "text"
      },
      "source": [
        "## **4.1 Save the preprocessed and transformed data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ5vVC2i1Yuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training data\n",
        "np.save(\n",
        "    '../data/text_train.npy',\n",
        "    text_train\n",
        ")\n",
        "np.save(\n",
        "    '../data/headline_train.npy',\n",
        "    headline_train\n",
        ")\n",
        "\n",
        "# Validation/dev data\n",
        "np.save(\n",
        "    '../data/text_val.npy',\n",
        "    text_val\n",
        ")\n",
        "np.save(\n",
        "    '../data/headline_val.npy',\n",
        "    headline_val\n",
        ")\n",
        "\n",
        "# Test data\n",
        "np.save(\n",
        "    '../data/text_test.npy',\n",
        "    text_test\n",
        ")\n",
        "np.save(\n",
        "    '../data/headline_test.npy',\n",
        "    headline_test\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYj2ixg5AsbK",
        "colab_type": "text"
      },
      "source": [
        "## **4.2 Save the lengths of input and target sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVrOtDExArxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training data\n",
        "np.save(\n",
        "    '../data/text_lengths_train.npy',\n",
        "    text_lengths_train\n",
        ")\n",
        "np.save(\n",
        "    '../data/headline_lengths_train.npy',\n",
        "    headline_lengths_train\n",
        ")\n",
        "\n",
        "# Validation/dev data\n",
        "np.save(\n",
        "    '../data/text_lengths_val.npy',\n",
        "    text_lengths_val\n",
        ")\n",
        "np.save(\n",
        "    '../data/headline_lengths_val.npy',\n",
        "    headline_lengths_val\n",
        ")\n",
        "\n",
        "# Test data\n",
        "np.save(\n",
        "    '../data/text_lengths_test.npy',\n",
        "    text_lengths_test\n",
        ")\n",
        "np.save(\n",
        "    '../data/headline_lengths_test.npy',\n",
        "    headline_lengths_test\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}