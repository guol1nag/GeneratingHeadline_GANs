{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating fake summaries for the discriminator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dixKtNdJhriq",
        "colab_type": "text"
      },
      "source": [
        "# **Headline Generation via Adversarial Training**\n",
        "## **Project for Statistical Natural Language Processing (COMP0087)**\n",
        "## **University College London**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**File: Generating fake summaries for the discriminator.ipynb**\n",
        "\n",
        "**Collaborators:**\n",
        "  - Daniel Stancl (ucabds7@ucl.ac.uk)\n",
        "  - Guoliang HE (ucabggh@ucl.ac.uk)\n",
        "  - Dorota Jagnesakova (ucabdj1@ucl.ac.uk)\n",
        "  - Zakhar Borok (zcabzbo@ucl.ac.uk)\n",
        "\n",
        "<hr>\n",
        "\n",
        "### **Description:** Colab notebook intented for the generation of fake summaries using by our pre-trained seq2seq models. There fake summaries are then used for thre pre-training of our discriminator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoJ3uJ4kiWlQ",
        "colab_type": "text"
      },
      "source": [
        "# **1 Setup**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- install and import libraries\n",
        "- remove and clone the most recent version of git repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-PdoqujiaW_",
        "colab_type": "text"
      },
      "source": [
        "## **1.1 GitHub stuff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBwA3aeAilFH",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.1 Set GitHub credentials and username of repo owner**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ay0We_0hj3G",
        "colab_type": "code",
        "outputId": "01455f3b-b41c-48c7-8708-755db4c9f46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# credentials\n",
        "user_email = '<your_email>'\n",
        "user = '<your_username>'\n",
        "user_password = \"<your_password>\"\n",
        "\n",
        "# username of repo owner\n",
        "owner_username = 'stancld'\n",
        "# reponame\n",
        "reponame = 'GeneratingHeadline_GANs'\n",
        "\n",
        "# generate \n",
        "add_origin_link = (\n",
        "    'https://{}:{}github@github.com/{}/{}.git'.format(\n",
        "    user, user_password, owner_username, reponame)\n",
        ")\n",
        "\n",
        "print(\"Link used for git cooperation:\\n{}\".format(add_origin_link))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Link used for git cooperation:\n",
            "https://<your_username>:<your_password>github@github.com/stancld/GeneratingHeadline_GANs.git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnAeQe-Pi5OP",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.2 Clone GitHub repo on the personal drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHJuTxPci46_",
        "colab_type": "code",
        "outputId": "1ed1695b-f407-43a6-936c-86a42e805938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "## Clone GitHub repo to the desired folder\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "%cd \"drive/My Drive/projects\"\n",
        "\n",
        "# Remove NLP_Project if presented and clone up-to-date repo\n",
        "!rm -r GeneratingHeadline_GANs\n",
        "!git clone https://github.com/stancld/GeneratingHeadline_GANs.git\n",
        "\n",
        "# Go to the NLP_Project folder\n",
        "%cd GeneratingHeadline_GANs\n",
        "\n",
        "# Config global user and add origin enabling us to execute push commands\n",
        "!git config --global user.email user_email\n",
        "!git remote rm origin\n",
        "!git remote add origin https://<your_username>:<your_password>@github.com/stancld/GeneratingHeadline_GANs.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/projects\n",
            "Cloning into 'GeneratingHeadline_GANs'...\n",
            "remote: Enumerating objects: 2311, done.\u001b[K\n",
            "remote: Total 2311 (delta 0), reused 0 (delta 0), pack-reused 2311\u001b[K\n",
            "Receiving objects: 100% (2311/2311), 19.33 MiB | 15.44 MiB/s, done.\n",
            "Resolving deltas: 100% (1501/1501), done.\n",
            "Checking out files: 100% (150/150), done.\n",
            "/content/drive/My Drive/projects/GeneratingHeadline_GANs\n",
            "/bin/bash: your_username: No such file or directory\n",
            "CPU times: user 221 ms, sys: 75 ms, total: 296 ms\n",
            "Wall time: 15.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q46T4fRjKWX",
        "colab_type": "text"
      },
      "source": [
        "### **1.1.3 Helper function: push_to_repo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFxVTx2VjKpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def push_to_repo():\n",
        "  \"\"\"\n",
        "  Helper function that pushes saved fils to github repo.\n",
        "  \"\"\"\n",
        "  !git remote rm origin\n",
        "  !git remote add origin https://<your_username>:<your_password>@github.com/stancld/GeneratingHeadline_GANs.git\n",
        "  !git checkout master\n",
        "  !git pull origin master\n",
        "  !git checkout models_branch\n",
        "  !git add .\n",
        "  !git commit -m \"model state update\"\n",
        "  !git checkout master\n",
        "  !git merge models_branch\n",
        "  !git push -u origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHA62OOqjKRd",
        "colab_type": "text"
      },
      "source": [
        "## **1.2 General stuff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H5CsJUHllW3",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.1 Install and import packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrocbitplhJR",
        "colab_type": "code",
        "outputId": "a55f4caf-49ab-4041-cb04-153f12efd648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pip install rouge==1.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge==1.0.0) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skBOwJhulq0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import copy\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from rouge import Rouge\n",
        "from termcolor import colored\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJUn6pJOlucH",
        "colab_type": "code",
        "outputId": "a7cfd049-a8ac-4762-c9aa-c85d3935bfba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d04hxK6_lyQ-",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.2 Set Torch device**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXS3k9IOl1MD",
        "colab_type": "code",
        "outputId": "9596d9ab-55be-4385-d9ed-a2111ea7d33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set torch.device to use GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.get_device_name())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAhy3doxl6uO",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.3 Run auxiliary Python scripts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc-jDg_Sl5V3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for transforming data to padded array\n",
        "run Code/data2PaddedArray.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaoUMhY79Z7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the generator\n",
        "run Code/Models/Attention_seq2seq.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKXH3I-w9ZrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the training class (generator)\n",
        "run Code/Models/generator_training_class.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAcLUlSARRBq",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.4 Necessary class for opening text & headline dictionaries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpU2YwWuRRcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class Language Dictionary\n",
        "class LangDict:\n",
        "  \"\"\"\n",
        "  Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "    self.n_words = 2\n",
        "\n",
        "  def add_article(self, article):\n",
        "    for word in article:\n",
        "      self.add_word(word)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmnNs5CW9hIF",
        "colab_type": "text"
      },
      "source": [
        "# **2. Load the data**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- Text_data & headline_data (splitted into train, dev and test set)\n",
        "- Pretrained GloVe embeddings\n",
        "- text and headline dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBPX_L5j-jKe",
        "colab_type": "text"
      },
      "source": [
        "## **2.1 WikiHow data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLS8qvNtJpSO",
        "colab_type": "text"
      },
      "source": [
        "### **2.1.1 Input nad target data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMb72dgo96dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train set\n",
        "text_train = np.load(\n",
        "    '../data/text_train.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "headline_train = np.load(\n",
        "    '../data/headline_train.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "\n",
        "# Dev set\n",
        "text_val = np.load(\n",
        "    '../data/text_val.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "headline_val = np.load(\n",
        "    '../data/headline_val.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "\n",
        "# Test set\n",
        "text_test = np.load(\n",
        "    '../data/text_test.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "headline_test = np.load(\n",
        "    '../data/headline_test.npy',\n",
        "    allow_pickle = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnaIzizJJwvw",
        "colab_type": "text"
      },
      "source": [
        "### **2.1.2 Lengths of the input and target data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVFnqCkDJwg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train set\n",
        "text_lengths_train = np.load(\n",
        "    '../data/text_lengths_train.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "headline_lengths_train = np.load(\n",
        "    '../data/headline_lengths_train.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "\n",
        "# Dev set\n",
        "text_lengths_val = np.load(\n",
        "    '../data/text_lengths_val.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "headline_lengths_val = np.load(\n",
        "    '../data/headline_lengths_val.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "\n",
        "# Test set\n",
        "text_lengths_test = np.load(\n",
        "    '../data/text_lengths_test.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "headline_lengths_test = np.load(\n",
        "    '../data/headline_lengths_test.npy',\n",
        "    allow_pickle = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br07eFIB-99H",
        "colab_type": "text"
      },
      "source": [
        "## **2.2 Filtered GloVe embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_LjEHCB_BCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embeddings for the text dictionry\n",
        "pre_train_weight = np.load(\n",
        "    '../data/embedding.npy'\n",
        ")\n",
        "\n",
        "# Embeddings for the headline dictionary\n",
        "pre_train_weight_head = np.load(\n",
        "    '../data/embedding_headline.npy'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG7p0DGN_xpC",
        "colab_type": "text"
      },
      "source": [
        "## **2.3 Headline & text dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlA9Ixxg_1kR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_dictionary\n",
        "with open('../data/text.dictionary', 'rb') as text_dictionary_file:\n",
        "  text_dictionary = pickle.load(text_dictionary_file)\n",
        "\n",
        "# headline_dictionary\n",
        "with open('../data/headline.dictionary', 'rb') as headline_dictionary_file:\n",
        "  headline_dictionary = pickle.load(headline_dictionary_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-dKXfYxLSRL",
        "colab_type": "text"
      },
      "source": [
        "# **3 Summary generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTz61IpbTzLU",
        "colab_type": "text"
      },
      "source": [
        "**Helper function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0SClJmSTxUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padded_hypotheses(x, threshold, pad_idx):\n",
        "  \"\"\"\n",
        "  :param x:\n",
        "    type:\n",
        "    description:\n",
        "  :param threshold:\n",
        "    type:\n",
        "    description:\n",
        "  :param pad_idx:\n",
        "    type:\n",
        "    description:\n",
        "\n",
        "  :return x:\n",
        "    type:\n",
        "    description  \n",
        "  \"\"\"\n",
        "  if x.shape[0] == threshold:\n",
        "    return x\n",
        "  else: \n",
        "    return np.r_[x, np.repeat(pad_idx, 32*(threshold - x.shape[0])).reshape(-1, 32)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgEL3JsjTztz",
        "colab_type": "text"
      },
      "source": [
        "**Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0FEGfKWLei1",
        "colab_type": "code",
        "outputId": "84fcb25f-6c39-452a-fb76-9fa9621f01ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# specify the size of the RNN layer\n",
        "best_model_size = 512\n",
        "\n",
        "##### Training specification #####\n",
        "grid = {'max_epochs': 25,\n",
        "        'batch_size': 32,\n",
        "        'learning_rate': 3e-4,\n",
        "        'clip': 10,\n",
        "        'l2_reg': 1e-4,\n",
        "        'model_name': \"generator{:.0f}\".format(best_model_size)\n",
        "      }\n",
        "#################################\n",
        "\n",
        "##### Model specification ######\n",
        "OUTPUT_DIM = len(headline_dictionary.index2word.keys()) # number of output classes\n",
        "ENC_EMB_DIM = pre_train_weight.shape[1] # embedding dimension\n",
        "ENC_HID_DIM = best_model_size # size of the RNN layer\n",
        "DEC_HID_DIM = best_model_size # size of the RNN layer\n",
        "\n",
        "enc_num_layers = 1 # number of layers in RNN\n",
        "dec_num_layers = 1 # number of layers in RNN\n",
        "\n",
        "ENC_DROPOUT = 0.1 # probability used for dropout in the encoder RNN unit\n",
        "DEC_DROPOUT = 0.1 # probability used for dropout in the decoder RNN unit\n",
        "#################################\n",
        "\n",
        "# Initialization\n",
        "Generator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid['l2_reg'], batch_size = grid['batch_size'],\n",
        "                    text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid['max_epochs'], learning_rate = grid['learning_rate'],\n",
        "                    clip = grid['clip'], teacher_forcing_ratio = 1, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\n",
        "                    DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, enc_num_layers = enc_num_layers, dec_num_layers = dec_num_layers,\n",
        "                    device = device, model_name = grid['model_name'], push_to_repo = push_to_repo)\n",
        "\n",
        "# Load the model\n",
        "Generator.load()\n",
        "\n",
        "##### TRAINING DATA #####\n",
        "hypotheses = Generator.generate_summaries(text_train, text_lengths_train, headline_train, headline_lengths_train)\n",
        "# Pad hypotheses\n",
        "hypotheses = np.concatenate(\n",
        "    [padded_hypotheses(hypothesis, 68, headline_dictionary.word2index['<pad>']) for hypothesis in hypotheses], axis = 1\n",
        ")\n",
        "# Correct the 'sos' symbol\n",
        "hypotheses[0, :] = 0\n",
        "# Concatenate real and fake summaries + transpose\n",
        "real_fake_train = np.concatenate((headline_train, hypotheses), axis = 1)\n",
        "real_fake_train = np.swapaxes(real_fake_train, 0, 1) # shape [n_examples, seq_len]\n",
        "# add labels as the first column - 1 = Real, 0 = Generated\n",
        "real_fake_train = np.c_[np.vstack((np.ones((headline_train.shape[1], 1)), np.zeros((hypotheses.shape[1], 1)))), real_fake_train]\n",
        "# save\n",
        "np.save('../data/real_fake_train.npy', real_fake_train)\n",
        "del hypotheses\n",
        "##########################\n",
        "\n",
        "\n",
        "##### VALIDATION DATA #####\n",
        "hypotheses = Generator.generate_summaries(text_val, text_lengths_val, headline_val, headline_lengths_val)\n",
        "# Pad hypotheses\n",
        "hypotheses = np.concatenate(\n",
        "    [padded_hypotheses(hypothesis, headline_val.shape[0], headline_dictionary.word2index['<pad>']) for hypothesis in hypotheses], axis = 1\n",
        ")\n",
        "# Correct the 'sos' symbol\n",
        "hypotheses[0, :] = 0\n",
        "# Concatenate real and fake summaries + transpose\n",
        "real_fake_val = np.concatenate((headline_val, hypotheses), axis = 1)\n",
        "real_fake_val = np.swapaxes(real_fake_train, 0, 1) # shape [n_examples, seq_len]\n",
        "# add labels as the first column - 1 = Real, 0 = Generated\n",
        "real_fake_val = np.c_[np.vstack((np.ones((headline_val.shape[1], 1)), np.zeros((hypotheses.shape[1], 1)))), real_fake_val]\n",
        "# reshuffle\n",
        "np.random.shuffle(real_fake_val)\n",
        "# save\n",
        "np.save('../data/real_fake_val.npy', real_fake_val)\n",
        "##########################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nfor model_size in model_sizes:\\n  \\n  ##### Training specification #####\\n  grid = {\\'max_epochs\\': 25,\\n          \\'batch_size\\': 32,\\n          \\'learning_rate\\': 3e-4,\\n          \\'clip\\': 10,\\n          \\'l2_reg\\': 1e-4,\\n          \\'model_name\\': \"generator{:.0f}\".format(model_size)\\n        }\\n  #################################\\n\\n  ##### Model specification ######\\n  OUTPUT_DIM = len(headline_dictionary.index2word.keys()) # number of output classes\\n  ENC_EMB_DIM = pre_train_weight.shape[1] # embedding dimension\\n  ENC_HID_DIM = model_size # size of the RNN layer\\n  DEC_HID_DIM = model_size # size of the RNN layer\\n\\n  enc_num_layers = 1 # number of layers in RNN\\n  dec_num_layers = 1 # number of layers in RNN\\n\\n  ENC_DROPOUT = 0.1 # probability used for dropout in the encoder RNN unit\\n  DEC_DROPOUT = 0.1 # probability used for dropout in the decoder RNN unit\\n\\n  # Initialization\\n  Generator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid[\\'l2_reg\\'], batch_size = grid[\\'batch_size\\'],\\n                      text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid[\\'max_epochs\\'], learning_rate = grid[\\'learning_rate\\'],\\n                      clip = grid[\\'clip\\'], teacher_forcing_ratio = 1, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\\n                      DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, enc_num_layers = enc_num_layers, dec_num_layers = dec_num_layers,\\n                      device = device, model_name = grid[\\'model_name\\'], push_to_repo = push_to_repo)\\n  \\n  # Load model if any\\n  Generator.load()\\n\\n  # Print model design and total number of parameters\\n  print(Generator.model)\\n  model_parameters = filter(lambda p: p.requires_grad, Generator.model.parameters())\\n  params = sum([np.prod(p.size()) for p in model_parameters])\\n  print(\"Model with RNN hidden size of {:.0f} has {:.0f} parameters in total.\".format(model_size, params))\\n\\n  # Run training\\n  Generator.train(X_train = text_train,\\n                y_train = headline_train,\\n                X_val = text_val,\\n                y_val = headline_val,\\n                X_train_lengths = text_lengths_train,\\n                y_train_lengths = headline_lengths_train,\\n                X_val_lengths = text_lengths_val,\\n                y_val_lengths = headline_lengths_val)\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}