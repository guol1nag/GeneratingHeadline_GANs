{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group22 - Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2H5CsJUHllW3",
        "d04hxK6_lyQ-",
        "BAhy3doxl6uO",
        "kAcLUlSARRBq",
        "TmnNs5CW9hIF",
        "LLS8qvNtJpSO",
        "hnaIzizJJwvw",
        "Br07eFIB-99H",
        "uG7p0DGN_xpC",
        "bA4FV7zM_VLN",
        "j8BPtCud_cZ6",
        "cZt32f4JqMsJ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dixKtNdJhriq",
        "colab_type": "text"
      },
      "source": [
        "# **Headline Generation via Adversarial Training**\n",
        "## **Project for Statistical Natural Language Processing (COMP0087)**\n",
        "## **University College London**\n",
        "\n",
        "<hr>\n",
        "\n",
        "**File: Group 22 - Demo.ipynb**\n",
        "\n",
        "**Collaborators:**\n",
        "  - Daniel Stancl (ucabds7@ucl.ac.uk)\n",
        "  - Guoliang HE (ucabggh@ucl.ac.uk)\n",
        "  - Dorota Jagnesakova (ucabdj1@ucl.ac.uk)\n",
        "  - Zakhar Borok (zcabzbo@ucl.ac.uk)\n",
        "\n",
        "<hr>\n",
        "\n",
        "### **Description:** ***To be added***\n",
        "\n",
        "<hr>\n",
        "\n",
        "### **README:** ***To be added***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoJ3uJ4kiWlQ",
        "colab_type": "text"
      },
      "source": [
        "# **1 Setup**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- install and import libraries\n",
        "- remove and clone the most recent version of git repository\n",
        "- run auxiliary Python scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-PdoqujiaW_",
        "colab_type": "text"
      },
      "source": [
        "## **1.1 GitHub stuff**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHJuTxPci46_",
        "colab_type": "code",
        "outputId": "dbc779f7-15e2-41f0-aff3-2a27cda23e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "## Clone GitHub repo to the desired folder\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "%cd \"drive/My Drive\"\n",
        "!rm -r Group22\n",
        "%mkdir \"Group22\"\n",
        "%cd \"Group22\"\n",
        "\n",
        "# Remove NLP_Project if presented and clone up-to-date repo\n",
        "!rm -r GeneratingHeadline_GANs\n",
        "!git clone https://github.com/stancld/GeneratingHeadline_GANs.git\n",
        "\n",
        "# Go to the NLP_Project folder\n",
        "%cd GeneratingHeadline_GANs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n",
            "/content/drive/My Drive/Group22\n",
            "rm: cannot remove 'GeneratingHeadline_GANs': No such file or directory\n",
            "Cloning into 'GeneratingHeadline_GANs'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 2357 (delta 26), reused 31 (delta 12), pack-reused 2311\u001b[K\n",
            "Receiving objects: 100% (2357/2357), 19.62 MiB | 20.20 MiB/s, done.\n",
            "Resolving deltas: 100% (1527/1527), done.\n",
            "Checking out files: 100% (153/153), done.\n",
            "/content/drive/My Drive/Group22/GeneratingHeadline_GANs\n",
            "CPU times: user 180 ms, sys: 67.7 ms, total: 247 ms\n",
            "Wall time: 10.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHA62OOqjKRd",
        "colab_type": "text"
      },
      "source": [
        "## **1.2 General stuff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H5CsJUHllW3",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.1 Install and import packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrocbitplhJR",
        "colab_type": "code",
        "outputId": "c5123f57-9385-42fd-8adb-c7998f456a13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "pip install rouge==1.0.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge==1.0.0) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skBOwJhulq0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import gc\n",
        "import copy\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from rouge import Rouge\n",
        "from termcolor import colored\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d04hxK6_lyQ-",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.2 Set Torch device**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXS3k9IOl1MD",
        "colab_type": "code",
        "outputId": "196184d6-399b-4ea6-9cba-accb6cd52a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Set torch.device to use GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.get_device_name())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAhy3doxl6uO",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.3 Run auxiliary Python scripts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc-jDg_Sl5V3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for transforming data to padded array\n",
        "run Code/data2PaddedArray.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaoUMhY79Z7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the generator\n",
        "run Code/Models/Attention_seq2seq.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKXH3I-w9ZrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the training class (generator)\n",
        "run Code/Models/generator_training_class.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5locvKqJghU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the discriminator\n",
        "run Code/Models/CNN_text_clf.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EloCDex4ghKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code for the training class (discriminator)\n",
        "run Code/Models/discriminator_training_class.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP81SaIbgg_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adversarial training\n",
        "run Code/Models/Adversarial_Training.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAcLUlSARRBq",
        "colab_type": "text"
      },
      "source": [
        "### **1.2.4 Necessary class for opening text & headline dictionaries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpU2YwWuRRcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class Language Dictionary\n",
        "class LangDict:\n",
        "  \"\"\"\n",
        "  Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "    self.n_words = 2\n",
        "\n",
        "  def add_article(self, article):\n",
        "    for word in article:\n",
        "      self.add_word(word)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmnNs5CW9hIF",
        "colab_type": "text"
      },
      "source": [
        "# **2. Load the data and models**\n",
        "\n",
        "<hr>\n",
        "\n",
        "- Text_data & headline_data (splitted into train, dev and test set)\n",
        "- Pretrained GloVe embeddings\n",
        "- text and headline dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OBTiUJf5brF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "839f8436-def2-4118-de83-839a6d444c03"
      },
      "source": [
        "# Downloadn all the data in ZIP file from Google Drive, unzip, remove\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1kv6gOwsHGE_H8wpYfjrIcQSRQuunzF2V' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1kv6gOwsHGE_H8wpYfjrIcQSRQuunzF2V\" -O DemoFiles.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip DemoFiles.zip\n",
        "!rm DemoFiles.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-29 12:00:07--  https://docs.google.com/uc?export=download&confirm=5RRJ&id=1kv6gOwsHGE_H8wpYfjrIcQSRQuunzF2V\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.20.139, 74.125.20.113, 74.125.20.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.20.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-70-docs.googleusercontent.com/docs/securesc/m6vl4ta55t70e3j9cgb6ku9u08lo8cc3/tk8r37rj9oa00oo3deftl3fl6soali60/1585483200000/01710675870877940700/11259274077832818668Z/1kv6gOwsHGE_H8wpYfjrIcQSRQuunzF2V?e=download [following]\n",
            "--2020-03-29 12:00:07--  https://doc-08-70-docs.googleusercontent.com/docs/securesc/m6vl4ta55t70e3j9cgb6ku9u08lo8cc3/tk8r37rj9oa00oo3deftl3fl6soali60/1585483200000/01710675870877940700/11259274077832818668Z/1kv6gOwsHGE_H8wpYfjrIcQSRQuunzF2V?e=download\n",
            "Resolving doc-08-70-docs.googleusercontent.com (doc-08-70-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-08-70-docs.googleusercontent.com (doc-08-70-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=9jfq03sfe8reo&continue=https://doc-08-70-docs.googleusercontent.com/docs/securesc/m6vl4ta55t70e3j9cgb6ku9u08lo8cc3/tk8r37rj9oa00oo3deftl3fl6soali60/1585483200000/01710675870877940700/11259274077832818668Z/1kv6gOwsHGE_H8wpYfjrIcQSRQuunzF2V?e%3Ddownload&hash=li2h6bpcc7ada90tqkpfnhdl0vq8q973 [following]\n",
            "--2020-03-29 12:00:07--  https://docs.google.com/nonceSigner?nonce=9jfq03sfe8reo&continue=https://doc-08-70-docs.googleusercontent.com/docs/securesc/m6vl4ta55t70e3j9cgb6ku9u08lo8cc3/tk8r37rj9oa00oo3deftl3fl6soali60/1585483200000/01710675870877940700/11259274077832818668Z/1kv6gOwsHGE_H8wpYfjrIcQSRQuunzF2V?e%3Ddownload&hash=li2h6bpcc7ada90tqkpfnhdl0vq8q973\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.20.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-08-70-docs.googleusercontent.com/docs/securesc/m6vl4ta55t70e3j9cgb6ku9u08lo8cc3/tk8r37rj9oa00oo3deftl3fl6soali60/1585483200000/01710675870877940700/11259274077832818668Z/1kv6gOwsHGE_H8wpYfjrIcQSRQuunzF2V?e=download&nonce=9jfq03sfe8reo&user=11259274077832818668Z&hash=rkc0f76svqf1omc6c43o0pd4edp2g10q [following]\n",
            "--2020-03-29 12:00:07--  https://doc-08-70-docs.googleusercontent.com/docs/securesc/m6vl4ta55t70e3j9cgb6ku9u08lo8cc3/tk8r37rj9oa00oo3deftl3fl6soali60/1585483200000/01710675870877940700/11259274077832818668Z/1kv6gOwsHGE_H8wpYfjrIcQSRQuunzF2V?e=download&nonce=9jfq03sfe8reo&user=11259274077832818668Z&hash=rkc0f76svqf1omc6c43o0pd4edp2g10q\n",
            "Connecting to doc-08-70-docs.googleusercontent.com (doc-08-70-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-zip-compressed]\n",
            "Saving to: ‘DemoFiles.zip’\n",
            "\n",
            "DemoFiles.zip           [            <=>     ] 372.18M  79.9MB/s    in 4.8s    \n",
            "\n",
            "2020-03-29 12:00:18 (77.8 MB/s) - ‘DemoFiles.zip’ saved [390261186]\n",
            "\n",
            "Archive:  DemoFiles.zip\n",
            "   creating: DemoFiles/\n",
            "  inflating: DemoFiles/embedding.npy  \n",
            "  inflating: DemoFiles/embedding_headline.npy  \n",
            "  inflating: DemoFiles/headline.dictionary  \n",
            "  inflating: DemoFiles/headline_lengths_test.npy  \n",
            "  inflating: DemoFiles/headline_test.npy  \n",
            "   creating: DemoFiles/Models/\n",
            "  inflating: DemoFiles/Models/Adversarial_v02.pth  \n",
            "  inflating: DemoFiles/Models/generator128.pth  \n",
            "  inflating: DemoFiles/Models/generator256.pth  \n",
            "  inflating: DemoFiles/Models/generator512.pth  \n",
            "  inflating: DemoFiles/README.txt    \n",
            "  inflating: DemoFiles/text.dictionary  \n",
            "  inflating: DemoFiles/text_lengths_test.npy  \n",
            "  inflating: DemoFiles/text_test.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBPX_L5j-jKe",
        "colab_type": "text"
      },
      "source": [
        "## **2.1 WikiHow data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLS8qvNtJpSO",
        "colab_type": "text"
      },
      "source": [
        "### **2.1.1 Input nad target data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMb72dgo96dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Paragraphs\n",
        "text_test = np.load(\n",
        "    'DemoFiles/text_test.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "\n",
        "# Headlines\n",
        "headline_test = np.load(\n",
        "    'DemoFiles/headline_test.npy',\n",
        "    allow_pickle = True\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnaIzizJJwvw",
        "colab_type": "text"
      },
      "source": [
        "### **2.1.2 Lengths of the input and target data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVFnqCkDJwg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_lengths_test = np.load(\n",
        "    'DemoFiles/text_lengths_test.npy',\n",
        "    allow_pickle = True\n",
        ")\n",
        "headline_lengths_test = np.load(\n",
        "    'DemoFiles/headline_lengths_test.npy',\n",
        "    allow_pickle = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br07eFIB-99H",
        "colab_type": "text"
      },
      "source": [
        "## **2.2 Filtered GloVe embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_LjEHCB_BCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embeddings for the text dictionry\n",
        "pre_train_weight = np.load(\n",
        "    'DemoFiles/embedding.npy'\n",
        ")\n",
        "\n",
        "# Embeddings for the headline dictionary\n",
        "pre_train_weight_head = np.load(\n",
        "    'DemoFiles/embedding_headline.npy'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG7p0DGN_xpC",
        "colab_type": "text"
      },
      "source": [
        "## **2.3 Headline & text dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlA9Ixxg_1kR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text_dictionary\n",
        "with open('DemoFiles/text.dictionary', 'rb') as text_dictionary_file:\n",
        "  text_dictionary = pickle.load(text_dictionary_file)\n",
        "\n",
        "# headline_dictionary\n",
        "with open('DemoFiles/headline.dictionary', 'rb') as headline_dictionary_file:\n",
        "  headline_dictionary = pickle.load(headline_dictionary_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-dKXfYxLSRL",
        "colab_type": "text"
      },
      "source": [
        "# **3 Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA4FV7zM_VLN",
        "colab_type": "text"
      },
      "source": [
        "## **3.0 Backend ROUGE computaiton**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0FEGfKWLei1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialization of Rouge() and define Helper function\n",
        "rouge = Rouge()\n",
        "def rouge_get_scores(hyp, ref, n):\n",
        "  try:\n",
        "    return float(rouge.get_scores(hyp, ref)[0]['rouge-{}'.format(n)]['f'])\n",
        "  except:\n",
        "    return \"drop\" # some summaries a flawed (there is nothing to compute) so they're dropped (only a negligible number of examples!! it does merely affect average scores by some thousandths)\n",
        "\n",
        "# Define indices of <pad> and <eos> tokens\n",
        "pad_idx = headline_dictionary.word2index['<pad>']\n",
        "eos_idx = headline_dictionary.word2index['eos']\n",
        "\n",
        "def push_to_repo():\n",
        "  \"\"\"\n",
        "  Helper function that pushes saved fils to github repo.\n",
        "  \"\"\"\n",
        "  !git remote rm origin\n",
        "  !git remote add origin https://<your_username>:<your_password>@github.com/stancld/GeneratingHeadline_GANs.git\n",
        "  !git checkout master\n",
        "  !git pull origin master\n",
        "  !git checkout models_branch\n",
        "  !git add .\n",
        "  !git commit -m \"model state update\"\n",
        "  !git checkout master\n",
        "  !git merge models_branch\n",
        "  !git push -u origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1WqGnscA_qH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_hypotheses(model_size, text_dictionary = text_dictionary, headline_dictionary = headline_dictionary,\n",
        "                        pre_train_weight = pre_train_weight, pre_train_weight_head = pre_train_weight_head,\n",
        "                        generator = generator, device = device, push_to_repo = push_to_repo):\n",
        "  grid = {'max_epochs': 25,\n",
        "          'batch_size': 32,\n",
        "          'learning_rate': 3e-4,\n",
        "          'clip': 10,\n",
        "          'l2_reg': 1e-4,\n",
        "          'model_name': \"generator{:.0f}\".format(model_size)\n",
        "        }\n",
        "\n",
        "  OUTPUT_DIM = len(headline_dictionary.index2word.keys()) # number of output classes\n",
        "  ENC_EMB_DIM = pre_train_weight.shape[1] # embedding dimension\n",
        "  ENC_HID_DIM = model_size # size of the RNN layer\n",
        "  DEC_HID_DIM = model_size # size of the RNN layer\n",
        "\n",
        "  enc_num_layers = 1 # number of layers in RNN\n",
        "  dec_num_layers = 1 # number of layers in RNN\n",
        "\n",
        "  ENC_DROPOUT = 0.1 # probability used for dropout in the encoder RNN unit\n",
        "  DEC_DROPOUT = 0.1 # probability used for dropout in the decoder RNN unit\n",
        "  ##########################################\n",
        "\n",
        "\n",
        "  # Initialization the model and load the state\n",
        "  Generator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid['l2_reg'], batch_size = grid['batch_size'],\n",
        "                      text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid['max_epochs'], learning_rate = grid['learning_rate'],\n",
        "                      clip = grid['clip'], teacher_forcing_ratio = 1, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\n",
        "                      DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, enc_num_layers = enc_num_layers, dec_num_layers = dec_num_layers,\n",
        "                      device = device, model_name = grid['model_name'], push_to_repo = push_to_repo)\n",
        "  Generator.load(demo = True)\n",
        "\n",
        "\n",
        "  ##### ROUGE scores #####\n",
        "  # Generate hypothesis and conver them into text\n",
        "  hypotheses = Generator.generate_summaries(text_test, text_lengths_test, headline_test, headline_lengths_test)\n",
        "  return hypotheses, Generator.n_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSB-iEBqBZFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_sizes = [128, 256, 512]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubrR0_c_EX4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert references into the text\n",
        "references = [' '.join([headline_dictionary.index2word[index] for index in headline_test[:, sentence] if (index != pad_idx) & (index != eos_idx)][1:]) for sentence in range(headline_test.shape[1])]\n",
        "first_run = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LDfJDPWMVb9",
        "colab_type": "text"
      },
      "source": [
        "## **3.1 Baseline models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PKqSl9Ak9Jq",
        "colab_type": "code",
        "outputId": "d799fd10-1fff-4931-94d6-79fd8d98b5c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "for model_size in model_sizes:\n",
        "  hypotheses, n_batches = baseline_hypotheses(model_size)\n",
        "  hypotheses = sum(\n",
        "      [[' '.join([headline_dictionary.index2word[index] for index in batch[:, hypothesis] if (index != pad_idx) & (index != eos_idx)][1:]) for hypothesis in range(batch.shape[1])] for batch in hypotheses], []\n",
        "  )\n",
        "  \n",
        "  # trim (only full batches are generated)\n",
        "  lim = int(32 * n_batches)\n",
        "  if first_run == 1:\n",
        "    references = references[:lim]\n",
        " \n",
        "  # Compute average rouge scores\n",
        "  rouge1 = [rouge_get_scores(hyp, ref, '1') for hyp, ref in zip(hypotheses, references)]\n",
        "  rouge1 = np.array([x for x in rouge1 if x != 'drop']).mean()\n",
        "  rouge2 = [rouge_get_scores(hyp, ref, '2') for hyp, ref in zip(hypotheses, references)]\n",
        "  rouge2 = np.array([x for x in rouge2 if x != 'drop']).mean()\n",
        "  rougel = [rouge_get_scores(hyp, ref, 'l') for hyp, ref in zip(hypotheses, references)]\n",
        "  rougel = np.array([x for x in rougel if x != 'drop']).mean()\n",
        "  \n",
        "  # cleaning\n",
        "  del hypotheses\n",
        "  gc.collect()\n",
        "\n",
        "  # Print rouge scores\n",
        "  print('Model size = {:.0f}.'.format(model_size))\n",
        "  print('ROUGE-1: {:.3f} on test data.'.format(100*np.array(rouge1)))\n",
        "  print('ROUGE-2: {:.3f} on test data.'.format(100*np.array(rouge2)))\n",
        "  print('ROUGE-l: {:.3f} on test data.'.format(100*np.array(rougel)))\n",
        "  print('---------------')\n",
        "  first_run += 1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model size = 128.\n",
            "ROUGE-1: 14.197 on test data.\n",
            "ROUGE-2: 3.168 on test data.\n",
            "ROUGE-l: 14.210 on test data.\n",
            "---------------\n",
            "Model size = 256.\n",
            "ROUGE-1: 15.283 on test data.\n",
            "ROUGE-2: 3.960 on test data.\n",
            "ROUGE-l: 15.312 on test data.\n",
            "---------------\n",
            "Model size = 512.\n",
            "ROUGE-1: 15.399 on test data.\n",
            "ROUGE-2: 4.353 on test data.\n",
            "ROUGE-l: 15.385 on test data.\n",
            "---------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZf6pN-dm6od",
        "colab_type": "text"
      },
      "source": [
        "## **3.2 SumGAN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8BPtCud_cZ6",
        "colab_type": "text"
      },
      "source": [
        "### **3.2.1 Backend Model initialization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9378RSJnRmN",
        "colab_type": "text"
      },
      "source": [
        "**Discriminator initialization**\n",
        "\n",
        "<hr>\n",
        "\n",
        "This step is required and it is an undesired artifact of our non-ideal implemntation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E133QKFVnROz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Model & Training specification #####\n",
        "param = {'max_epochs': 80,\n",
        "            'learning_rate': 5e-4,\n",
        "            'batch_size': 32,               \n",
        "            'seq_len': 68,                   # length of your summary\n",
        "            'embed_dim': 200,\n",
        "            'drop_out': 0.5,\n",
        "            'kernel_num': 50,                 # number of your feature map\n",
        "            'in_channel': 1,                 # for text classification should be one\n",
        "            # how many conv net are used in parallel in text classification\n",
        "            'parallel_layer': 3,\n",
        "            'model_name': 'n_{:.0f}_d_{:.0f}'.format(50, 10*0.5),\n",
        "            'device':'cuda'}\n",
        "##########################################\n",
        "\n",
        "\n",
        "# Initialization the model and load the state\n",
        "Discriminator = Discriminator_utility(pre_train_weight_head,**param)\n",
        "Discriminator.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHO8l48uCa1t",
        "colab_type": "text"
      },
      "source": [
        "**Generator initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJEFOpQACar3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_size = 512\n",
        "\n",
        "grid = {'max_epochs': 25,\n",
        "        'batch_size': 32,\n",
        "        'learning_rate': 3e-4,\n",
        "        'clip': 10,\n",
        "        'l2_reg': 1e-4,\n",
        "        'model_name': \"generator{:.0f}\".format(model_size)\n",
        "      }\n",
        "\n",
        "OUTPUT_DIM = len(headline_dictionary.index2word.keys()) # number of output classes\n",
        "ENC_EMB_DIM = pre_train_weight.shape[1] # embedding dimension\n",
        "ENC_HID_DIM = model_size # size of the RNN layer\n",
        "DEC_HID_DIM = model_size # size of the RNN layer\n",
        "\n",
        "enc_num_layers = 1 # number of layers in RNN\n",
        "dec_num_layers = 1 # number of layers in RNN\n",
        "\n",
        "ENC_DROPOUT = 0.1 # probability used for dropout in the encoder RNN unit\n",
        "DEC_DROPOUT = 0.1 # probability used for dropout in the decoder RNN unit\n",
        "##########################################\n",
        "\n",
        "\n",
        "# Initialization the model and load the state\n",
        "Generator = generator(model = _Seq2Seq, loss_function = nn.CrossEntropyLoss, optimiser = optim.Adam, l2_reg = grid['l2_reg'], batch_size = grid['batch_size'],\n",
        "                    text_dictionary = text_dictionary, embeddings = pre_train_weight, max_epochs = grid['max_epochs'], learning_rate = grid['learning_rate'],\n",
        "                    clip = grid['clip'], teacher_forcing_ratio = 1, OUTPUT_DIM = OUTPUT_DIM, ENC_HID_DIM = ENC_HID_DIM, ENC_EMB_DIM = ENC_EMB_DIM,\n",
        "                    DEC_HID_DIM = DEC_HID_DIM, ENC_DROPOUT = ENC_DROPOUT, DEC_DROPOUT = DEC_DROPOUT, enc_num_layers = enc_num_layers, dec_num_layers = dec_num_layers,\n",
        "                    device = device, model_name = grid['model_name'], push_to_repo = push_to_repo)\n",
        "Generator.load(demo = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx9mNLKNoz-S",
        "colab_type": "text"
      },
      "source": [
        "**GAN initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1LCYtN0ozue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Model & Training specification #####\n",
        "grid = {'max_epochs': 2,\n",
        "        'batch_size': 32,\n",
        "        'learning_rate_D': 5e-4,\n",
        "        'learning_rate_G': 1e-3,\n",
        "        'G_multiple': 2,\n",
        "        'l2_reg': 0.1,      # appled only for discriminator\n",
        "        'clip': 10,\n",
        "        'model_name': 'Adversarial_v02',\n",
        "        'text_dictionary': text_dictionary,\n",
        "        'headline_dictionary': headline_dictionary,\n",
        "        'device': 'cuda',\n",
        "        'noise_std': 0.00, \n",
        "        'optim_d_prob': 0.15 #this values determines the probability a discriminator will make step with\n",
        "        }\n",
        "\n",
        "##########################################\n",
        "\n",
        "\n",
        "# Initialization the model and load the state\n",
        "GAN = AdversarialTraining(Generator, Discriminator, optim.Adam, optim.SGD, **grid)\n",
        "GAN.load(demo = True)  # if any"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxEcTkNKp02C",
        "colab_type": "text"
      },
      "source": [
        "### **3.2.2 ROUGE**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDhsxq6gmcuX",
        "colab_type": "code",
        "outputId": "8a8d217a-a904-490f-ed32-fd1917cb590e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "##### ROUGE scores #####\n",
        "# Generate hypothesis and conver them into text\n",
        "hypotheses = GAN.generate_summaries(text_test, text_lengths_test, headline_test, headline_lengths_test)\n",
        "hypotheses = sum(\n",
        "    [[' '.join([headline_dictionary.index2word[index] for index in batch[:, hypothesis] if (index != pad_idx) & (index != eos_idx)][1:]) for hypothesis in range(batch.shape[1])] for batch in hypotheses], []\n",
        ")\n",
        "\n",
        "# Convert references into the text\n",
        "references = [' '.join([headline_dictionary.index2word[index] for index in headline_test[:, sentence] if (index != pad_idx) & (index != eos_idx)][1:]) for sentence in range(headline_test.shape[1])]\n",
        "\n",
        "# trim (only full batches are generated)\n",
        "lim = int(32 * n_batches)\n",
        "references = references[:lim]\n",
        "\n",
        "# Compute average rouge scores\n",
        "rouge1 = [rouge_get_scores(hyp, ref, '1') for hyp, ref in zip(hypotheses, references)]\n",
        "rouge1 = np.array([x for x in rouge1 if x != 'drop']).mean()\n",
        "rouge2 = [rouge_get_scores(hyp, ref, '2') for hyp, ref in zip(hypotheses, references)]\n",
        "rouge2 = np.array([x for x in rouge2 if x != 'drop']).mean()\n",
        "rougel = [rouge_get_scores(hyp, ref, 'l') for hyp, ref in zip(hypotheses, references)]\n",
        "rougel = np.array([x for x in rougel if x != 'drop']).mean()\n",
        "\n",
        "# cleaning\n",
        "del hypotheses, references\n",
        "gc.collect()\n",
        "\n",
        "# Print rouge scores\n",
        "print('Model size = {:.0f}.'.format(model_size))\n",
        "print('ROUGE-1: {:.3f} on test data.'.format(100*np.array(rouge1)))\n",
        "print('ROUGE-2: {:.3f} on test data.'.format(100*np.array(rouge2)))\n",
        "print('ROUGE-l: {:.3f} on test data.'.format(100*np.array(rougel)))\n",
        "print('---------------')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model size = 512.\n",
            "ROUGE-1: 19.382 on test data.\n",
            "ROUGE-2: 6.209 on test data.\n",
            "ROUGE-l: 19.343 on test data.\n",
            "---------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZt32f4JqMsJ",
        "colab_type": "text"
      },
      "source": [
        "## **Backend sampling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbourzaVqwQF",
        "colab_type": "text"
      },
      "source": [
        "**Baseline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0Z0v9viqTp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Generator.load(demo = True)\n",
        "\n",
        "hypotheses_2 = Generator.generate_summaries(text_test, text_lengths_test, headline_test, headline_lengths_test)\n",
        "hypotheses_2 = sum(\n",
        "    [[' '.join([headline_dictionary.index2word[index] for index in batch[:, hypothesis] if (index != pad_idx) & (index != eos_idx)][1:]) for hypothesis in range(batch.shape[1])] for batch in hypotheses_2], []\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHU3LhEXq31V",
        "colab_type": "text"
      },
      "source": [
        "**SumGAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1QXGxXRq3qB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAN.load(demo = True)\n",
        "\n",
        "hypotheses_1 = GAN.generate_summaries(text_test, text_lengths_test, headline_test, headline_lengths_test)\n",
        "hypotheses_1 = sum(\n",
        "    [[' '.join([headline_dictionary.index2word[index] for index in batch[:, hypothesis] if (index != pad_idx) & (index != eos_idx)][1:]) for hypothesis in range(batch.shape[1])] for batch in hypotheses_1], []\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8clYhjdgq-FK",
        "colab_type": "text"
      },
      "source": [
        "**Take reference headlines**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mep59Aygq-6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "references = [' '.join([headline_dictionary.index2word[index] for index in headline_test[:, sentence] if (index != pad_idx) & (index != eos_idx)][1:]) for sentence in range(headline_test.shape[1])]\n",
        "# trim\n",
        "n_batches = len(references) // grid['batch_size']\n",
        "lim = n_batches * grid['batch_size']\n",
        "references = references[:lim]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7021moxfrGKK",
        "colab_type": "text"
      },
      "source": [
        "## **Print random samples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOXHDmRirGcq",
        "colab_type": "code",
        "outputId": "f35f0289-58b6-4e86-8cc0-10eb4b9ae922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Take a random sample\n",
        "samples = np.random.choice(lim, 50, replace = False)\n",
        "hypotheses_1, hypotheses_2, references = (\n",
        "    np.array(hypotheses_1)[samples],\n",
        "    np.array(hypotheses_2)[samples],\n",
        "    np.array(references)[samples]\n",
        ")\n",
        "\n",
        "# Print and save\n",
        "for hyp_1, hyp_2, ref in zip(\n",
        "    hypotheses_1,\n",
        "    hypotheses_2,\n",
        "    references):\n",
        "  print(f'Reference: {ref}')\n",
        "  print(colored(f'Baseline: {hyp_2}', 'blue'))\n",
        "  print(colored(f'SumGAN: {hyp_1}', 'green')) \n",
        "  print('----------')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reference: rely on friends and family .\n",
            "\u001b[34mBaseline: be positive .\u001b[0m\n",
            "\u001b[32mSumGAN: ask for help .\u001b[0m\n",
            "----------\n",
            "Reference: fuel your body with nutritious foods .\n",
            "\u001b[34mBaseline: eat a healthy diet .\u001b[0m\n",
            "\u001b[32mSumGAN: eat a healthy diet .\u001b[0m\n",
            "----------\n",
            "Reference: have your vet trim your guinea pigs nails .\n",
            "\u001b[34mBaseline: get your guinea pig .\u001b[0m\n",
            "\u001b[32mSumGAN: get your guinea pig .\u001b[0m\n",
            "----------\n",
            "Reference: use washable bedding on the bed .\n",
            "\u001b[34mBaseline: clean your bedding .\u001b[0m\n",
            "\u001b[32mSumGAN: clean your bedding .\u001b[0m\n",
            "----------\n",
            "Reference: get a good night s rest .\n",
            "\u001b[34mBaseline: get plenty of sleep .\u001b[0m\n",
            "\u001b[32mSumGAN: get enough sleep .\u001b[0m\n",
            "----------\n",
            "Reference: consider your family wishes and lifestyle .\n",
            "\u001b[34mBaseline: be aware of your family .\u001b[0m\n",
            "\u001b[32mSumGAN: consider your family history .\u001b[0m\n",
            "----------\n",
            "Reference: add additional front matter .\n",
            "\u001b[34mBaseline: write your first draft .\u001b[0m\n",
            "\u001b[32mSumGAN: write a brief .\u001b[0m\n",
            "----------\n",
            "Reference: use ice for injuries .\n",
            "\u001b[34mBaseline: ice your ice .\u001b[0m\n",
            "\u001b[32mSumGAN: ice your ice .\u001b[0m\n",
            "----------\n",
            "Reference: spend time with people who support your recovery .\n",
            "\u001b[34mBaseline: join your support .\u001b[0m\n",
            "\u001b[32mSumGAN: join a support group .\u001b[0m\n",
            "----------\n",
            "Reference: look for severe swelling around your thumb .\n",
            "\u001b[34mBaseline: continue your fingers .\u001b[0m\n",
            "\u001b[32mSumGAN: continue your fingers .\u001b[0m\n",
            "----------\n",
            "Reference: get involved on campus .\n",
            "\u001b[34mBaseline: join a share .\u001b[0m\n",
            "\u001b[32mSumGAN: join a local community .\u001b[0m\n",
            "----------\n",
            "Reference: keep a notepad handy .\n",
            "\u001b[34mBaseline: keep a journal .\u001b[0m\n",
            "\u001b[32mSumGAN: write down your thoughts .\u001b[0m\n",
            "----------\n",
            "Reference: keep a journal of your mistakes and successes .\n",
            "\u001b[34mBaseline: keep a journal .\u001b[0m\n",
            "\u001b[32mSumGAN: write down your goals .\u001b[0m\n",
            "----------\n",
            "Reference: talk about yourself too .\n",
            "\u001b[34mBaseline: talk to your friend .\u001b[0m\n",
            "\u001b[32mSumGAN: talk to your friend about your friends .\u001b[0m\n",
            "----------\n",
            "Reference: practice .\n",
            "\u001b[34mBaseline: keep your mind .\u001b[0m\n",
            "\u001b[32mSumGAN: practice your speech .\u001b[0m\n",
            "----------\n",
            "Reference: start with one outdoor run a week .\n",
            "\u001b[34mBaseline: run your workout .\u001b[0m\n",
            "\u001b[32mSumGAN: run a workout .\u001b[0m\n",
            "----------\n",
            "Reference: view all logs recorded by your fitbit tracker .\n",
            "\u001b[34mBaseline: click the app .\u001b[0m\n",
            "\u001b[32mSumGAN: choose the right side .\u001b[0m\n",
            "----------\n",
            "Reference: research local laws .\n",
            "\u001b[34mBaseline: avoid share .\u001b[0m\n",
            "\u001b[32mSumGAN: check the laws .\u001b[0m\n",
            "----------\n",
            "Reference: use the self evaluation as an opportunity .\n",
            "\u001b[34mBaseline: be patient .\u001b[0m\n",
            "\u001b[32mSumGAN: identify the purpose of your job .\u001b[0m\n",
            "----------\n",
            "Reference: use an ice cube or a cool wet compress .\n",
            "\u001b[34mBaseline: apply a ice compress .\u001b[0m\n",
            "\u001b[32mSumGAN: apply a cold compress .\u001b[0m\n",
            "----------\n",
            "Reference: adopt good habits .\n",
            "\u001b[34mBaseline: avoid eating .\u001b[0m\n",
            "\u001b[32mSumGAN: avoid eating habits .\u001b[0m\n",
            "----------\n",
            "Reference: identify common early symptoms .\n",
            "\u001b[34mBaseline: recognize the symptoms .\u001b[0m\n",
            "\u001b[32mSumGAN: recognize the symptoms of symptoms .\u001b[0m\n",
            "----------\n",
            "Reference: determine what the nursing school is looking for from your personal statement .\n",
            "\u001b[34mBaseline: follow the instructions .\u001b[0m\n",
            "\u001b[32mSumGAN: follow the instructions s instructions .\u001b[0m\n",
            "----------\n",
            "Reference: attend parent management training programs and family therapy .while you may have found it less difficult to handle your other children and their problems you may find yourself at a loss in how to parent your child with odd .\n",
            "\u001b[34mBaseline: learn about your options .\u001b[0m\n",
            "\u001b[32mSumGAN: learn a new class .\u001b[0m\n",
            "----------\n",
            "Reference: take deep steady breaths .\n",
            "\u001b[34mBaseline: take a deep breath .\u001b[0m\n",
            "\u001b[32mSumGAN: take a deep breath .\u001b[0m\n",
            "----------\n",
            "Reference: keep a chair at arms length in front of you .\n",
            "\u001b[34mBaseline: get a good position .\u001b[0m\n",
            "\u001b[32mSumGAN: get a good posture .\u001b[0m\n",
            "----------\n",
            "Reference: practice safe sex .\n",
            "\u001b[34mBaseline: avoid using your condom .\u001b[0m\n",
            "\u001b[32mSumGAN: avoid using sex .\u001b[0m\n",
            "----------\n",
            "Reference: inquire about long term prognosis .\n",
            "\u001b[34mBaseline: apply a prescription .\u001b[0m\n",
            "\u001b[32mSumGAN: consider your dentist .\u001b[0m\n",
            "----------\n",
            "Reference: strengthen your relationships with other people .\n",
            "\u001b[34mBaseline: be happy .\u001b[0m\n",
            "\u001b[32mSumGAN: think about what you want to do .\u001b[0m\n",
            "----------\n",
            "Reference: revise and format the article .\n",
            "\u001b[34mBaseline: type the article .\u001b[0m\n",
            "\u001b[32mSumGAN: write the article .\u001b[0m\n",
            "----------\n",
            "Reference: try singing .\n",
            "\u001b[34mBaseline: enjoy your child .\u001b[0m\n",
            "\u001b[32mSumGAN: listen to your baby .\u001b[0m\n",
            "----------\n",
            "Reference: choose a suitable target .\n",
            "\u001b[34mBaseline: choose a share .\u001b[0m\n",
            "\u001b[32mSumGAN: choose a share .\u001b[0m\n",
            "----------\n",
            "Reference: strap the cage down .\n",
            "\u001b[34mBaseline: wear a comfortable cage .\u001b[0m\n",
            "\u001b[32mSumGAN: use a share or share .\u001b[0m\n",
            "----------\n",
            "Reference: pay attention to share relationships .\n",
            "\u001b[34mBaseline: create a share .\u001b[0m\n",
            "\u001b[32mSumGAN: write down the share .\u001b[0m\n",
            "----------\n",
            "Reference: stay in a cool room .\n",
            "\u001b[34mBaseline: turn on the air .\u001b[0m\n",
            "\u001b[32mSumGAN: keep the air cool .\u001b[0m\n",
            "----------\n",
            "Reference: consider other immune boosting supplements .\n",
            "\u001b[34mBaseline: take vitamin c .\u001b[0m\n",
            "\u001b[32mSumGAN: take vitamin c . vitamin c is a day .\u001b[0m\n",
            "----------\n",
            "Reference: add your contact information .\n",
            "\u001b[34mBaseline: type the name of the page .\u001b[0m\n",
            "\u001b[32mSumGAN: address the letter .\u001b[0m\n",
            "----------\n",
            "Reference: spend time with friends .\n",
            "\u001b[34mBaseline: enjoy your friends .\u001b[0m\n",
            "\u001b[32mSumGAN: join a support group .\u001b[0m\n",
            "----------\n",
            "Reference: receive confirmation of processing .\n",
            "\u001b[34mBaseline: submit your application .\u001b[0m\n",
            "\u001b[32mSumGAN: submit your application .\u001b[0m\n",
            "----------\n",
            "Reference: aim for the sweet spot on the backboard .\n",
            "\u001b[34mBaseline: turn the ball .\u001b[0m\n",
            "\u001b[32mSumGAN: place the ball in the ball .\u001b[0m\n",
            "----------\n",
            "Reference: begin by building one room .\n",
            "\u001b[34mBaseline: take a quiet .\u001b[0m\n",
            "\u001b[32mSumGAN: take a quiet place .\u001b[0m\n",
            "----------\n",
            "Reference: talk about strengths .\n",
            "\u001b[34mBaseline: be confident .\u001b[0m\n",
            "\u001b[32mSumGAN: talk to your team .\u001b[0m\n",
            "----------\n",
            "Reference: address your partners fears .\n",
            "\u001b[34mBaseline: continue to be patient .\u001b[0m\n",
            "\u001b[32mSumGAN: address your partner .\u001b[0m\n",
            "----------\n",
            "Reference: know how infection is transmitted .\n",
            "\u001b[34mBaseline: avoid alcohol .\u001b[0m\n",
            "\u001b[32mSumGAN: avoid certain and fluids .\u001b[0m\n",
            "----------\n",
            "Reference: limit sun exposure .\n",
            "\u001b[34mBaseline: avoid using the share .\u001b[0m\n",
            "\u001b[32mSumGAN: avoid using the skin .\u001b[0m\n",
            "----------\n",
            "Reference: try the two share method .\n",
            "\u001b[34mBaseline: drink water .\u001b[0m\n",
            "\u001b[32mSumGAN: drink a glass of water .\u001b[0m\n",
            "----------\n",
            "Reference: visit the dentist regularly .\n",
            "\u001b[34mBaseline: visit your dentist .\u001b[0m\n",
            "\u001b[32mSumGAN: visit your dentist .\u001b[0m\n",
            "----------\n",
            "Reference: contact your doctor if symptoms do not go away quickly .\n",
            "\u001b[34mBaseline: drink plenty of water .\u001b[0m\n",
            "\u001b[32mSumGAN: see your doctor .\u001b[0m\n",
            "----------\n",
            "Reference: specify which pages the article appears on .\n",
            "\u001b[34mBaseline: type the page of the article .\u001b[0m\n",
            "\u001b[32mSumGAN: cite the page numbers .\u001b[0m\n",
            "----------\n",
            "Reference: provide share clues .\n",
            "\u001b[34mBaseline: create a story .\u001b[0m\n",
            "\u001b[32mSumGAN: make a list of your story .\u001b[0m\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsXnHQwvrJ8N",
        "colab_type": "text"
      },
      "source": [
        "# **CLEANING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaQrEBirJyg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4bd94943-104b-4887-eadf-5808a0a5b65b"
      },
      "source": [
        "%cd ..\n",
        "%cd .. \n",
        "!rm -r Group22"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Group22\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}