{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_discriminator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c39KzsJgQmw_",
        "colab_type": "code",
        "outputId": "162109b7-aeff-4c39-df55-358d3247fd1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import sklearn.model_selection as sk_ModelSelection\n",
        "#increase field limit to read embedding\n",
        "import sys\n",
        "!git clone https://github.com/guol1nag/GeneratingHeadline_GANs.git\n",
        "%cd GeneratingHeadline_GANs\n",
        "%run ./Code/Models/CNN_text_clf.py\n",
        "%run ./Code/Models/discriminator_training_class.py\n",
        "%run ./Code/data2PaddedArray.py\n",
        "%run ./Code/text_preprocessing.py\n",
        "%run ./Code/contractions.py\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.get_device_name() if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GeneratingHeadline_GANs'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 486 (delta 23), reused 42 (delta 11), pack-reused 429\u001b[K\n",
            "Receiving objects: 100% (486/486), 16.98 MiB | 37.56 MiB/s, done.\n",
            "Resolving deltas: 100% (217/217), done.\n",
            "/content/GeneratingHeadline_GANs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ybNOKU4RQuK",
        "colab_type": "code",
        "outputId": "a3a6bb51-072d-4ff9-e543-1e839009ac02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ewkZnSMcm5c",
        "colab_type": "code",
        "outputId": "7a0ee1d2-6725-45aa-f9aa-5f7c1adfb0ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = r'/content/drive/My Drive/pre_train_weight.csv'\n",
        "pre_train_weight = np.loadtxt(path)\n",
        "gc.collect()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tp5IwwhRQYy",
        "colab_type": "text"
      },
      "source": [
        "# pre-train embedding & pre-processing\n",
        "\n",
        "pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrW5LskadHyg",
        "colab_type": "code",
        "outputId": "cc20c91c-43ed-4d37-e18a-46ba03f0a9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%time\n",
        "data = pd.read_csv('/content/drive/My Drive/wikihowSep.csv',\n",
        "                   error_bad_lines = False).astype(str)\n",
        "print(data.shape)\n",
        "\n",
        "for item in ['text', 'headline']:\n",
        "  exec(\"\"\"{}_data = text_preprocessing(data=data, item = '{}', contraction_map=CONTRACTION_MAP,\n",
        "                                  drop_digits=False, remove_stopwords=False, stemming=False)\"\"\".format(item, item),\n",
        "       locals(), globals()\n",
        "  )\n",
        "\n",
        "max_examples = 150000\n",
        "max_threshold = 0.75\n",
        "\n",
        "# drop examples with an invalid ratio of length of text and headline\n",
        "text_len = [len(t) for t in text_data]\n",
        "head_len = [len(h) for h in headline_data]\n",
        "\n",
        "ratio = [h/t for t, h in zip(text_len, head_len)]\n",
        "\n",
        "problems1 = [problem for problem, r in enumerate(ratio) if (r > max_threshold)]\n",
        "text_data, headline_data = np.delete(text_data, problems1), np.delete(headline_data, problems1)\n",
        "print(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\n",
        "\n",
        "# drop too long articles (to avoid struggles with CUDA memory) and too short\n",
        "text_len = [len(t) for t in text_data]\n",
        "\n",
        "problems2 = [problem for problem, text_length in enumerate(text_len) if ((text_length > 200) | (text_length < 10) )]\n",
        "text_data, headline_data = np.delete(text_data, problems2), np.delete(headline_data, problems2)\n",
        "print(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\n",
        "\n",
        "# drop too pairs with too short/long summaries\n",
        "head_len = [len(h) for h in headline_data]\n",
        "\n",
        "problems3 = [problem for problem, headline_len in enumerate(head_len) if ( (headline_len > 75) | (headline_len < 2) )]\n",
        "text_data, headline_data = np.delete(text_data, problems3), np.delete(headline_data, problems3)\n",
        "print(\"Number of examples after filtering: {:.0f}\".format(text_data.shape[0]))\n",
        "\n",
        "# some cleaning\n",
        "del text_len, head_len, ratio, problems1, problems2, problems3\n",
        "gc.collect()\n",
        "\n",
        "\"\"\"\n",
        "# trim the data to have only a subset of the data for our project\n",
        "try:\n",
        "  data = data[:max_examples]\n",
        "except:\n",
        "  pass\n",
        "\"\"\"\n",
        "# drop examples with an invalid ratio of length of text and headline\n",
        "text_len = [len(t) for t in text_data]\n",
        "head_len = [len(h) for h in headline_data]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1585695, 5)\n",
            "Number of examples after filtering: 1259273\n",
            "Number of examples after filtering: 1214567\n",
            "Number of examples after filtering: 1214535\n",
            "CPU times: user 4min 34s, sys: 8.18 s, total: 4min 42s\n",
            "Wall time: 4min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSvDhOAlqULy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(222)\n",
        "\n",
        "split = np.random.uniform(0, 1, size = text_data.shape[0])\n",
        "\n",
        "# Train set\n",
        "text_train, headline_train = text_data[split <= 0.9], headline_data[split <= 0.9]\n",
        "# Validation set\n",
        "text_val, headline_val = text_data[(split > 0.9) & (split <= 0.95)], headline_data[(split > 0.9) & (split <= 0.95)]\n",
        "# Test set\n",
        "text_test, headline_test = text_data[split > 0.95], headline_data[split > 0.95]\n",
        "\n",
        "del data\n",
        "gc.collect()\n",
        "\n",
        "def sort_data(text, headline):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  headline = np.array(\n",
        "      [y for x,y in sorted(zip(text, headline), key = lambda pair: len(pair[0]), reverse = True)]\n",
        "  )\n",
        "  text = list(text)\n",
        "  text.sort(key = lambda x: len(x), reverse = True)\n",
        "  text = np.array(text)\n",
        "\n",
        "  ####### i just want 100 samples!! ###########\n",
        "  return text[:100], headline[:100]\n",
        "\n",
        "# Train set\n",
        "text_train, headline_train = sort_data(text_train, headline_train)\n",
        "# Validation set\n",
        "text_val, headline_val = sort_data(text_val, headline_val)\n",
        "# Test set\n",
        "text_test, headline_test = sort_data(text_test, headline_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy8eHaSAj3vD",
        "colab_type": "code",
        "outputId": "0fafa09f-14aa-48c5-e491-1af891788ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "class LangDict:\n",
        "  \"\"\"\n",
        "  Source: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "    self.n_words = 2\n",
        "\n",
        "  def add_article(self, article):\n",
        "    for word in article:\n",
        "      self.add_word(word)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "# Create dictionary based on the training data\n",
        "text_dictionary = LangDict()\n",
        "headline_dictionary = LangDict()\n",
        "\n",
        "for article in text_train:\n",
        "  text_dictionary.add_article(article)\n",
        "\n",
        "for article in headline_train:\n",
        "  headline_dictionary.add_article(article)\n",
        "print(\"There are {:.0f} distinct words in the untrimmed text dictionary\".format(len(text_dictionary.word2index.keys())))\n",
        "print(\"There are {:.0f} distinct words in the untrimmed headline dictionary\".format(len(headline_dictionary.word2index.keys())))\n",
        "\n",
        "# Trim a dictionary to the words with at least 10 occurences within the text\n",
        "text_min_count = 1\n",
        "head_min_count = 2\n",
        "\n",
        "## TEXT DICTIONARY\n",
        "subset_words = [word for (word, count) in text_dictionary.word2count.items() if count >= text_min_count]\n",
        "text_dictionary.word2index = {word: i for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "text_dictionary.index2word = {i: word for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "text_dictionary.word2count = {word: count for (word, count) in text_dictionary.word2count.items() if count >= text_min_count}\n",
        "\n",
        "## HEADLINE DICTIONARY\n",
        "subset_words = [word for (word, count) in headline_dictionary.word2count.items() if count >= head_min_count]\n",
        "headline_dictionary.word2index = {word: i for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "headline_dictionary.index2word = {i: word for (word, i) in zip(subset_words, range(len(subset_words)))}\n",
        "headline_dictionary.word2count = {word: count for (word, count) in headline_dictionary.word2count.items() if count >= head_min_count}\n",
        "\n",
        "print(\"There are {:.0f} distinct words in the trimmed text dictionary, where only word with at least {:.0f} occurences are retained\".format(len(text_dictionary.word2index.keys()), text_min_count))\n",
        "print(\"There are {:.0f} distinct words in the trimmed headline dictionary, where only word with at least {:.0f} occurences are retained\".format(len(headline_dictionary.word2index.keys()), head_min_count))\n",
        "del text_min_count, head_min_count, subset_words\n",
        "## TEXT DICTIONARY\n",
        "pad_idx = max(list(text_dictionary.index2word.keys())) + 1\n",
        "\n",
        "text_dictionary.word2index['<pad>'] = pad_idx\n",
        "text_dictionary.index2word[pad_idx] = '<pad>'\n",
        "\n",
        "print(len(text_dictionary.index2word.keys()))\n",
        "\n",
        "## HEADLINE DICTIONARY\n",
        "pad_idx = max(list(headline_dictionary.index2word.keys())) + 1\n",
        "\n",
        "headline_dictionary.word2index['<pad>'] = pad_idx\n",
        "headline_dictionary.index2word[pad_idx] = '<pad>'\n",
        "\n",
        "print(len(headline_dictionary.index2word.keys()))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 3573 distinct words in the untrimmed text dictionary\n",
            "There are 342 distinct words in the untrimmed headline dictionary\n",
            "There are 3573 distinct words in the trimmed text dictionary, where only word with at least 1 occurences are retained\n",
            "There are 76 distinct words in the trimmed headline dictionary, where only word with at least 2 occurences are retained\n",
            "3574\n",
            "77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp8jBbRVl7Do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train set\n",
        "text_train, text_lengths_train, headline_train, headline_lengths_train = data2PaddedArray(text_train, headline_train, {'text_dictionary': text_dictionary,\n",
        "                                                                                                                       'headline_dictionary': headline_dictionary},\n",
        "                                                                                          pre_train_weight)\n",
        "# Validation set\n",
        "text_val, text_lengths_val, headline_val, headline_lengths_val = data2PaddedArray(text_val, headline_val, {'text_dictionary': text_dictionary,\n",
        "                                                                                                           'headline_dictionary': headline_dictionary},\n",
        "                                                                                  pre_train_weight)\n",
        "# Test set\n",
        "text_test, text_lengths_test, headline_test, headline_lengths_test = data2PaddedArray(text_test, headline_test, {'text_dictionary': text_dictionary,\n",
        "                                                                                                                 'headline_dictionary': headline_dictionary},\n",
        "                                                                                       pre_train_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSIpdIMhTW-P",
        "colab_type": "text"
      },
      "source": [
        "##### have a look at data, make X and y for discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDsjwoLZIKCz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0c7c93bc-f427-4f8d-b607-dc2e9b0398a5"
      },
      "source": [
        "print('number of sample; length of summary')\n",
        "X_train = torch.from_numpy(np.transpose(headline_train)).long()\n",
        "X_train.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of sample; length of summary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 33])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Dyga9yF0oQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b11f220e-daab-4faf-aebf-5a500bee1f18"
      },
      "source": [
        "y_train = (torch.rand(100) > 0.5).long()\n",
        "y_train.size()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5QeMH8ORamz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQMME-QZpTbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator_utility.show_parameter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt7uNvKhHZem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param = {'max_epochs':64,\n",
        "        'learning_rate':1e-3,\n",
        "        'batch_size':5,               \n",
        "        'seq_len': 20,                   # length of your summary\n",
        "        'embed_dim': 200,\n",
        "        'drop_out': 0,\n",
        "        'kernel_num': 5,                 # number of your feature map\n",
        "        'in_channel': 1,                 # for text classification should be one\n",
        "        # how many conv net are used in parallel in text classification\n",
        "        'parallel_layer':3,\n",
        "        'model_name': 'discriminator',\n",
        "        'device':device}\n",
        "embedding = pre_train_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkekKBW-IdeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drt = Discriminator_utility(embedding,**param)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM6tbCZWQJ9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee95c63f-5794-4015-9434-29a9f474f228"
      },
      "source": [
        "drt.run_epochs(X_train,y_train,X_test = X_train,y_test = y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1:\n",
            "Train Loss: 14.112\n",
            "Validation Loss: 13.381\n",
            "Epoch: 2:\n",
            "Train Loss: 13.291\n",
            "Validation Loss: 12.713\n",
            "Epoch: 3:\n",
            "Train Loss: 12.620\n",
            "Validation Loss: 12.142\n",
            "Epoch: 4:\n",
            "Train Loss: 12.064\n",
            "Validation Loss: 11.481\n",
            "Epoch: 5:\n",
            "Train Loss: 11.411\n",
            "Validation Loss: 10.838\n",
            "Epoch: 6:\n",
            "Train Loss: 10.747\n",
            "Validation Loss: 10.160\n",
            "Epoch: 7:\n",
            "Train Loss: 10.125\n",
            "Validation Loss: 9.538\n",
            "Epoch: 8:\n",
            "Train Loss: 9.493\n",
            "Validation Loss: 8.953\n",
            "Epoch: 9:\n",
            "Train Loss: 8.936\n",
            "Validation Loss: 8.306\n",
            "Epoch: 10:\n",
            "Train Loss: 8.323\n",
            "Validation Loss: 7.739\n",
            "Epoch: 11:\n",
            "Train Loss: 7.772\n",
            "Validation Loss: 7.245\n",
            "Epoch: 12:\n",
            "Train Loss: 7.341\n",
            "Validation Loss: 6.861\n",
            "Epoch: 13:\n",
            "Train Loss: 6.911\n",
            "Validation Loss: 6.597\n",
            "Epoch: 14:\n",
            "Train Loss: 6.652\n",
            "Validation Loss: 6.183\n",
            "Epoch: 15:\n",
            "Train Loss: 6.301\n",
            "Validation Loss: 5.720\n",
            "Epoch: 16:\n",
            "Train Loss: 5.868\n",
            "Validation Loss: 5.450\n",
            "Epoch: 17:\n",
            "Train Loss: 5.585\n",
            "Validation Loss: 5.254\n",
            "Epoch: 18:\n",
            "Train Loss: 5.367\n",
            "Validation Loss: 4.981\n",
            "Epoch: 19:\n",
            "Train Loss: 5.125\n",
            "Validation Loss: 4.724\n",
            "Epoch: 20:\n",
            "Train Loss: 4.916\n",
            "Validation Loss: 4.594\n",
            "Epoch: 21:\n",
            "Train Loss: 4.717\n",
            "Validation Loss: 4.436\n",
            "Epoch: 22:\n",
            "Train Loss: 4.580\n",
            "Validation Loss: 4.263\n",
            "Epoch: 23:\n",
            "Train Loss: 4.450\n",
            "Validation Loss: 4.068\n",
            "Epoch: 24:\n",
            "Train Loss: 4.239\n",
            "Validation Loss: 3.914\n",
            "Epoch: 25:\n",
            "Train Loss: 4.093\n",
            "Validation Loss: 3.790\n",
            "Epoch: 26:\n",
            "Train Loss: 3.985\n",
            "Validation Loss: 3.659\n",
            "Epoch: 27:\n",
            "Train Loss: 3.851\n",
            "Validation Loss: 3.533\n",
            "Epoch: 28:\n",
            "Train Loss: 3.715\n",
            "Validation Loss: 3.456\n",
            "Epoch: 29:\n",
            "Train Loss: 3.640\n",
            "Validation Loss: 3.371\n",
            "Epoch: 30:\n",
            "Train Loss: 3.559\n",
            "Validation Loss: 3.263\n",
            "Epoch: 31:\n",
            "Train Loss: 3.451\n",
            "Validation Loss: 3.172\n",
            "Epoch: 32:\n",
            "Train Loss: 3.361\n",
            "Validation Loss: 3.065\n",
            "Epoch: 33:\n",
            "Train Loss: 3.254\n",
            "Validation Loss: 3.020\n",
            "Epoch: 34:\n",
            "Train Loss: 3.220\n",
            "Validation Loss: 2.913\n",
            "Epoch: 35:\n",
            "Train Loss: 3.112\n",
            "Validation Loss: 2.825\n",
            "Epoch: 36:\n",
            "Train Loss: 3.006\n",
            "Validation Loss: 2.768\n",
            "Epoch: 37:\n",
            "Train Loss: 2.950\n",
            "Validation Loss: 2.693\n",
            "Epoch: 38:\n",
            "Train Loss: 2.895\n",
            "Validation Loss: 2.627\n",
            "Epoch: 39:\n",
            "Train Loss: 2.803\n",
            "Validation Loss: 2.531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ebe738a3d377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/GeneratingHeadline_GANs/Code/Models/discriminator_training_class.py\u001b[0m in \u001b[0;36mrun_epochs\u001b[0;34m(self, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GeneratingHeadline_GANs/Code/Models/discriminator_training_class.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlossfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvjPkLdNW5oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}